import os
import streamlit as st
import time
from collections import defaultdict, deque
import json

# ==================== è½»é‡çº§å¯¼å…¥ ====================
PROJECT_DIR = "/root/autodl-tmp/Fun-ASR"
TEMP_DIR = os.path.join(PROJECT_DIR, "temp")
VOICEPRINT_DIR = os.path.join(PROJECT_DIR, "voiceprints")
HOTWORD_DIR = os.path.join(PROJECT_DIR, "hotwords")
LM_CACHE_DIR = os.path.join(PROJECT_DIR, "lm_cache")

os.makedirs(TEMP_DIR, exist_ok=True)
os.makedirs(VOICEPRINT_DIR, exist_ok=True)
os.makedirs(HOTWORD_DIR, exist_ok=True)
os.makedirs(LM_CACHE_DIR, exist_ok=True)

# ==================== é¡µé¢é…ç½® ====================
st.set_page_config(
    page_title="AudioTrans",
    page_icon="ğŸ™ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("AudioTrans")

# ==================== å»¶è¿Ÿå¯¼å…¥å‡½æ•° ====================
def lazy_import_heavy_libs():
    """å»¶è¿Ÿå¯¼å…¥é‡å‹åº“"""
    global np, torch, sf, AutoModel, warnings
    global HDBSCAN, cosine, Counter, re, noisereduce
    global PCA, StandardScaler, heapq
    
    import warnings
    warnings.filterwarnings("ignore")
    
    import numpy as np
    import torch
    import soundfile as sf
    from funasr import AutoModel
    from scipy.spatial.distance import cosine
    from collections import Counter
    import re
    import heapq
    
    # éŸ³é¢‘å¢å¼º
    try:
        import noisereduce
    except:
        noisereduce = None
    
    # èšç±»
    try:
        from hdbscan import HDBSCAN
    except:
        from sklearn.cluster import DBSCAN as HDBSCAN
    
    # é™ç»´
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler
    
    return np, torch, sf, AutoModel, HDBSCAN, cosine, Counter, re, noisereduce, PCA, StandardScaler, heapq

# ==================== æ£€æŸ¥æ˜¯å¦å·²å¯¼å…¥ ====================
if 'libs_loaded' not in st.session_state:
    st.session_state.libs_loaded = False

if not st.session_state.libs_loaded:
    st.success("ç³»ç»Ÿå·²å°±ç»ª!")
    
    col1, col2, col3 = st.columns(3)
    with col2:
        if st.button("åŠ è½½AIå¼•æ“", type="primary", use_container_width=True):
            with st.spinner("æ­£åœ¨å¯¼å…¥AIæ¨¡å—..."):
                lazy_import_heavy_libs()
                st.session_state.libs_loaded = True
                st.rerun()
    
    st.info("ç‚¹å‡»æŒ‰é’®å¼€å§‹åŠ è½½æ¨¡å‹")
    st.stop()

# ==================== å¯¼å…¥æˆåŠŸå,åŠ è½½æ ¸å¿ƒåŠŸèƒ½ ====================

np, torch, sf, AutoModel, HDBSCAN, cosine, Counter, re, noisereduce, PCA, StandardScaler, heapq = lazy_import_heavy_libs()

# è®¾ç½®è®¾å¤‡
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

def tensor_to_numpy(data):
    if torch.is_tensor(data):
        return data.detach().cpu().numpy()
    elif isinstance(data, np.ndarray):
        return data
    return np.array(data)

def normalize_embedding(emb):
    emb = tensor_to_numpy(emb).flatten()
    norm = np.linalg.norm(emb)
    return emb / (norm + 1e-8)

# ==================== å·¥ä¸šçº§é‡å¤æŠ‘åˆ¶å™¨ ====================

class IndustrialRepetitionSuppressor:
    """
    å·¥ä¸šçº§é‡å¤æŠ‘åˆ¶å™¨
    """
    
    @staticmethod
    def aggressive_dedup(text):
        if not text or len(text) < 2:
            return text
        
        original_len = len(text)
        
        # ç¬¬ä¸€å±‚ï¼šé•¿æ¨¡å¼å»é‡ (20å­—â†’3å­—)
        for pattern_len in range(20, 2, -1):
            text = IndustrialRepetitionSuppressor._remove_all_repetitions(text, pattern_len)
        
        # ç¬¬äºŒå±‚ï¼šå­—ç¬¦çº§å»é‡
        text = IndustrialRepetitionSuppressor._remove_char_repetitions(text)
        
        # ç¬¬ä¸‰å±‚ï¼šè¯­ä¹‰çº§å»é‡
        text = IndustrialRepetitionSuppressor._remove_semantic_repetitions(text)
        
        # ç¬¬å››å±‚ï¼šæ»‘åŠ¨çª—å£æ‰«æ
        text = IndustrialRepetitionSuppressor._sliding_window_dedup(text)
        
        return text
    
    @staticmethod
    def _remove_all_repetitions(text, pattern_len):
        if len(text) < pattern_len * 2:
            return text
        
        result = []
        i = 0
        
        while i < len(text):
            if i + pattern_len > len(text):
                result.append(text[i:])
                break
            
            pattern = text[i:i + pattern_len]
            
            j = i + pattern_len
            repeat_count = 1
            
            while j + pattern_len <= len(text) and text[j:j + pattern_len] == pattern:
                repeat_count += 1
                j += pattern_len
            
            if repeat_count > 1:
                result.append(pattern)
                i = j
            else:
                result.append(pattern[0])
                i += 1
        
        return ''.join(result)
    
    @staticmethod
    def _remove_char_repetitions(text, max_repeat=2):
        if not text:
            return text
        
        result = []
        prev_char = None
        count = 0
        
        for char in text:
            if char == prev_char:
                count += 1
                if count < max_repeat:
                    result.append(char)
            else:
                result.append(char)
                prev_char = char
                count = 1
        
        return ''.join(result)
    
    @staticmethod
    def _remove_semantic_repetitions(text):
        if len(text) < 10:
            return text
        
        sentences = []
        current = ""
        
        for char in text:
            current += char
            if char in 'ã€‚,!?;':
                if current.strip():
                    sentences.append(current.strip())
                current = ""
        
        if current.strip():
            sentences.append(current.strip())
        
        unique_sentences = []
        for sent in sentences:
            is_similar = False
            for existing in unique_sentences:
                similarity = IndustrialRepetitionSuppressor._compute_similarity(sent, existing)
                if similarity > 0.7:
                    is_similar = True
                    break
            
            if not is_similar:
                unique_sentences.append(sent)
        
        return ''.join(unique_sentences)
    
    @staticmethod
    def _compute_similarity(text1, text2):
        if not text1 or not text2:
            return 0.0
        
        set1 = set(text1)
        set2 = set(text2)
        
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        
        return intersection / union if union > 0 else 0.0
    
    @staticmethod
    def _sliding_window_dedup(text, window_size=10):
        if len(text) < window_size * 2:
            return text
        
        result = []
        seen_windows = {}
        
        for i in range(len(text)):
            window_end = min(i + window_size, len(text))
            window = text[i:window_end]
            
            if window in seen_windows:
                last_pos = seen_windows[window]
                if i - last_pos < window_size * 2:
                    continue
            
            result.append(text[i])
            seen_windows[window] = i
        
        return ''.join(result)
    
    @staticmethod
    def detect_and_fix_segments(segments):
        if not segments:
            return segments
        
        cleaned_segments = []
        seen_texts = set()
        
        for seg in segments:
            original_text = seg['text']
            original_len = len(original_text)
            
            cleaned_text = IndustrialRepetitionSuppressor.aggressive_dedup(original_text)
            
            if cleaned_text in seen_texts:
                continue
            
            seen_texts.add(cleaned_text)
            
            cleanup_ratio = 1 - (len(cleaned_text) / max(1, original_len))
            
            if cleanup_ratio > 0.8:
                seg['confidence'] = seg.get('confidence', 0.8) * 0.2
            elif cleanup_ratio > 0.5:
                seg['confidence'] = seg.get('confidence', 0.8) * 0.5
            elif cleanup_ratio > 0.3:
                seg['confidence'] = seg.get('confidence', 0.8) * 0.7
            
            seg['text'] = cleaned_text
            
            if len(cleaned_text) >= 2:
                cleaned_segments.append(seg)
        
        return cleaned_segments

# ==================== ASRè§£ç é²æ£’æ€§å¢å¼º ====================

class RobustASRDecoder:
    """
    é²æ£’ASRè§£ç å™¨
    """
    
    @staticmethod
    def decode_with_repetition_penalty(asr_model, audio_path, penalty=1.5):
        try:
            res = asr_model.generate(
                input=audio_path,
                batch_size_s=300,
                device=DEVICE
            )
            
            if not res:
                return [], 0.0
            
            asr_results = res if isinstance(res, list) else [res]
            candidates = []
            
            for item in asr_results:
                text = ""
                conf = 0.8
                
                if isinstance(item, dict):
                    text = item.get("text", "").strip()
                    conf = item.get("confidence", 0.8)
                elif hasattr(item, 'text'):
                    text = item.text.strip()
                
                if text:
                    repetition_score = RobustASRDecoder._detect_repetition_in_text(text)
                    adjusted_conf = conf * (1.0 - repetition_score)
                    
                    candidates.append((text, adjusted_conf))
            
            candidates.sort(key=lambda x: x[1], reverse=True)
            
            return candidates
        
        except Exception as e:
            return []
    
    @staticmethod
    def _detect_repetition_in_text(text):
        if len(text) < 6:
            return 0.0
        
        max_repetition = 0
        
        for pattern_len in range(10, 2, -1):
            if len(text) < pattern_len * 2:
                continue
            
            for i in range(len(text) - pattern_len):
                pattern = text[i:i + pattern_len]
                count = text.count(pattern)
                
                if count > 1:
                    repetition_ratio = (count * pattern_len) / len(text)
                    max_repetition = max(max_repetition, repetition_ratio)
        
        return min(1.0, max_repetition)

# ==================== è‡ªé€‚åº”å‚æ•°è°ƒæ•´ ====================

class AdaptiveParameterTuner:
    """
    è‡ªé€‚åº”å‚æ•°è°ƒæ•´å™¨
    """
    
    @staticmethod
    def analyze_audio_profile(audio, sr):
        duration = len(audio) / sr
        
        rms = np.sqrt(np.mean(audio**2))
        
        dynamic_range = np.max(np.abs(audio)) - np.min(np.abs(audio))
        
        zero_crossings = np.sum(np.abs(np.diff(np.sign(audio)))) / (2 * len(audio))
        
        energy = audio ** 2
        silence_ratio = np.sum(energy < np.mean(energy) * 0.1) / len(energy)
        
        profile = {
            'duration': duration,
            'rms': rms,
            'dynamic_range': dynamic_range,
            'zero_crossings': zero_crossings,
            'silence_ratio': silence_ratio
        }
        
        return profile
    
    @staticmethod
    def tune_parameters(audio_profile):
        params = {
            'vad_threshold': 0.5,
            'min_segment_duration': 1.5,
            'max_segment_duration': 30,
            'denoise_strength': 0.85,
            'lm_weight': 0.3,
            'beam_size': 5,
            'confidence_threshold': 0.65
        }
        
        if audio_profile['silence_ratio'] > 0.5:
            params['vad_threshold'] = 0.4
        elif audio_profile['silence_ratio'] < 0.2:
            params['vad_threshold'] = 0.6
        
        if audio_profile['dynamic_range'] < 0.3:
            params['denoise_strength'] = 0.90
        elif audio_profile['dynamic_range'] > 0.7:
            params['denoise_strength'] = 0.75
        
        if audio_profile['duration'] > 3600:
            params['max_segment_duration'] = 25
            params['min_segment_duration'] = 2.0
        elif audio_profile['duration'] < 60:
            params['max_segment_duration'] = 60
            params['min_segment_duration'] = 1.0
        
        if audio_profile['zero_crossings'] < 0.05:
            params['beam_size'] = 8
            params['lm_weight'] = 0.4
        elif audio_profile['zero_crossings'] > 0.15:
            params['beam_size'] = 3
            params['lm_weight'] = 0.2
        
        return params

# ==================== æ‰¹å¤„ç†åŠ é€Ÿå™¨ ====================

class BatchProcessor:
    """
    æ‰¹å¤„ç†åŠ é€Ÿå™¨
    """
    
    @staticmethod
    def batch_extract_embeddings(audio_segments, sr, sv_model, batch_size=5):
        embeddings = []
        temp_paths = []
        
        try:
            for idx, seg in enumerate(audio_segments):
                temp_path = os.path.join(TEMP_DIR, f"batch_emb_{idx}_{int(time.time()*1000)}.wav")
                sf.write(temp_path, seg['audio'], sr)
                temp_paths.append(temp_path)
            
            for i in range(0, len(temp_paths), batch_size):
                batch_paths = temp_paths[i:i + batch_size]
                
                for path in batch_paths:
                    try:
                        res = sv_model.generate(input=path)
                        if res and isinstance(res, list) and len(res) > 0:
                            item = res[0]
                            if isinstance(item, dict):
                                for key in ["embedding", "spk_embedding", "emb"]:
                                    if key in item:
                                        embeddings.append(tensor_to_numpy(item[key]))
                                        break
                                else:
                                    embeddings.append(None)
                            elif hasattr(item, 'embedding'):
                                embeddings.append(tensor_to_numpy(item.embedding))
                            else:
                                embeddings.append(None)
                        else:
                            embeddings.append(None)
                    except:
                        embeddings.append(None)
            
        finally:
            for path in temp_paths:
                if os.path.exists(path):
                    os.remove(path)
        
        return embeddings

# ==================== ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¯­è¨€æ¨¡å‹èåˆ ====================

class ContextAwareLanguageModel:
    """ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¯­è¨€æ¨¡å‹ - å•†ä¸šçº§LMèåˆ"""
    def __init__(self, lm_weight=0.4, context_window=3):
        self.lm_weight = lm_weight
        self.context_window = context_window
        self.bigram_probs = self._load_bigram_model()
        self.unigram_probs = self._load_unigram_model()
        self.trigram_probs = self._load_trigram_model()
        self.context_history = []

    def _load_bigram_model(self):
        bigrams = {
            ('æˆ‘', 'æ˜¯'): 0.15, ('ä½ ', 'å¥½'): 0.12, ('ä»€', 'ä¹ˆ'): 0.10,
            ('æ€', 'ä¹ˆ'): 0.09, ('è¿™', 'ä¸ª'): 0.11, ('é‚£', 'ä¸ª'): 0.08,
            ('å¯', 'ä»¥'): 0.13, ('ä¸', 'æ˜¯'): 0.07, ('æ²¡', 'æœ‰'): 0.09,
            ('å·²', 'ç»'): 0.06, ('è¦', 'æ±‚'): 0.08, ('å‘', 'å±•'): 0.07,
            ('è¿›', 'è¡Œ'): 0.09, ('å»º', 'è®¾'): 0.06, ('æ', 'é«˜'): 0.08,
            ('åŠ ', 'å¼º'): 0.07, ('ä¿', 'æŒ'): 0.06, ('è½', 'å®'): 0.05,
        }
        return bigrams

    def _load_unigram_model(self):
        unigrams = {
            'çš„': 0.07, 'äº†': 0.04, 'æ˜¯': 0.03, 'åœ¨': 0.03,
            'æˆ‘': 0.03, 'æœ‰': 0.02, 'å’Œ': 0.02, 'äºº': 0.02,
            'è¿™': 0.02, 'ä¸­': 0.02, 'å¤§': 0.01, 'ä¸º': 0.01,
            'å›½': 0.01, 'å®¶': 0.01, 'æ°‘': 0.01, 'ä¸»': 0.01,
            'æ”¿': 0.01, 'åºœ': 0.01, 'ç¤¾': 0.01, 'ä¼š': 0.01,
        }
        return unigrams

    def _load_trigram_model(self):
        trigrams = {
            ('æˆ‘', 'ä»¬', 'è¦'): 0.05, ('ä¸­', 'å›½', 'æ¢¦'): 0.04,
            ('æ”¹', 'é©', 'å¼€'): 0.03, ('ç¤¾', 'ä¼š', 'ä¸»'): 0.03,
            ('äºº', 'æ°‘', 'ç¾¤'): 0.03, ('ç§‘', 'å­¦', 'å‘'): 0.03,
        }
        return trigrams

    def compute_lm_score(self, text, context=None):
        if not text or len(text) < 1:
            return 0.0

        score = 0.0
        chars = list(text)

        # åŸºç¡€è¯­è¨€æ¨¡å‹è¯„åˆ†
        for i in range(len(chars) - 2):
            trigram = (chars[i], chars[i+1], chars[i+2])
            if trigram in self.trigram_probs:
                score += np.log(self.trigram_probs[trigram] + 1e-8) * 1.5
            else:
                bigram = (chars[i], chars[i+1])
                if bigram in self.bigram_probs:
                    score += np.log(self.bigram_probs[bigram] + 1e-8)
                else:
                    if chars[i] in self.unigram_probs:
                        score += np.log(self.unigram_probs[chars[i]] + 1e-8) * 0.4

        # ä¸Šä¸‹æ–‡ç›¸å…³æ€§è¯„åˆ†
        if context and len(self.context_history) > 0:
            context_score = self._compute_context_relevance(text, context)
            score += context_score * 0.3

        score = score / max(1, len(chars))
        return score

    def _compute_context_relevance(self, text, current_context):
        """è®¡ç®—æ–‡æœ¬ä¸ä¸Šä¸‹æ–‡çš„ç›¸å…³æ€§"""
        if not text or not current_context:
            return 0.0

        # å…³é”®è¯é‡å åº¦
        text_words = set(text)
        context_words = set(''.join(current_context))

        intersection = len(text_words & context_words)
        union = len(text_words | context_words)

        if union == 0:
            return 0.0

        # Jaccardç›¸ä¼¼åº¦
        jaccard = intersection / union

        # ä¸»é¢˜è¿ç»­æ€§å¥–åŠ±
        continuity_bonus = self._compute_topic_continuity(text, current_context)

        return jaccard + continuity_bonus

    def _compute_topic_continuity(self, text, context):
        """è®¡ç®—ä¸»é¢˜è¿ç»­æ€§"""
        # ç®€å•çš„ä¸»é¢˜è¯åŒ¹é…
        topic_words = ['å‘å±•', 'å»ºè®¾', 'æ”¹é©', 'åˆ›æ–°', 'ç§‘æŠ€', 'æ•™è‚²', 'ç»æµ']
        text_topic_score = sum(1 for word in topic_words if word in text)
        context_topic_score = sum(1 for segment in context for word in topic_words if word in segment)

        if context_topic_score > 0:
            return (text_topic_score / len(topic_words)) * 0.2
        return 0.0

    def fuse_scores(self, asr_score, text, context=None):
        lm_score = self.compute_lm_score(text, context)
        fused_score = asr_score + self.lm_weight * lm_score

        # æ›´æ–°ä¸Šä¸‹æ–‡å†å²
        self.context_history.append(text)
        if len(self.context_history) > self.context_window:
            self.context_history.pop(0)

        return fused_score

    def get_context(self):
        return self.context_history.copy()

# ==================== ROVERå¤šå€™é€‰èåˆç³»ç»Ÿ ====================

class ROVERFusionSystem:
    """ROVER (Recognizer Output Voting Error Reduction) - å•†ä¸šçº§å¤šæ¨¡å‹èåˆ"""
    def __init__(self, models, voting_weights=None):
        self.models = models  # å¤šä¸ªASRæ¨¡å‹
        self.voting_weights = voting_weights or [1.0] * len(models)
        self.time_aligner = TimeAlignmentSystem()
        self.confidence_fuser = ConfidenceFusionEngine()

    def rover_fusion(self, audio_path, progress_callback=None):
        """æ‰§è¡ŒROVERèåˆè§£ç """
        if not self.models:
            return []

        # ç¬¬ä¸€é˜¶æ®µï¼šå¤šæ¨¡å‹å¹¶è¡Œè§£ç 
        if progress_callback:
            progress_callback("å¤šæ¨¡å‹å¹¶è¡Œè§£ç ...")

        all_candidates = []
        for i, model in enumerate(self.models):
            try:
                if progress_callback:
                    progress_callback(f"æ¨¡å‹ {i+1}/{len(self.models)} è§£ç ä¸­...")

                candidates = self._decode_with_model(model, audio_path)
                all_candidates.append(candidates)
            except Exception as e:
                print(f"Model {i} decoding failed: {e}")
                all_candidates.append([])

        # ç¬¬äºŒé˜¶æ®µï¼šæ—¶é—´å¯¹é½
        if progress_callback:
            progress_callback("æ—¶é—´å¯¹é½ä¸è¯å›¾ç”Ÿæˆ...")

        aligned_candidates = self.time_aligner.align_candidates(all_candidates)

        # ç¬¬ä¸‰é˜¶æ®µï¼šROVERæŠ•ç¥¨èåˆ
        if progress_callback:
            progress_callback("ROVERæŠ•ç¥¨èåˆ...")

        fused_results = self._rover_vote(aligned_candidates)

        return fused_results

    def _decode_with_model(self, model, audio_path):
        """ä½¿ç”¨å•ä¸ªæ¨¡å‹è§£ç """
        try:
            # ç”Ÿæˆå¤šä¸ªå€™é€‰ç»“æœï¼ˆé€šè¿‡ä¸åŒå‚æ•°ï¼‰
            candidates = []

            # æ ‡å‡†è§£ç 
            res = model.generate(
                input=audio_path,
                batch_size_s=300,
                device=DEVICE
            )

            if res:
                asr_results = res if isinstance(res, list) else [res]
                for item in asr_results:
                    text = ""
                    conf = 0.8

                    if isinstance(item, dict):
                        text = item.get("text", "").strip()
                        conf = item.get("confidence", 0.8)
                    elif hasattr(item, 'text'):
                        text = item.text.strip()

                    if text:
                        candidates.append({
                            'text': text,
                            'confidence': conf,
                            'model_id': id(model),
                            'start_time': 0.0,  # ç®€åŒ–ç‰ˆï¼Œå®é™…éœ€è¦VADä¿¡æ¯
                            'end_time': 0.0
                        })

            return candidates

        except Exception as e:
            print(f"Model decoding failed: {e}")
            return []

    def _rover_vote(self, aligned_candidates):
        """ROVERæŠ•ç¥¨æœºåˆ¶"""
        if not aligned_candidates:
            return []

        # ç®€åŒ–çš„ROVERå®ç°
        fused_results = []

        # å¯¹æ¯ä¸ªæ—¶é—´æ®µè¿›è¡ŒæŠ•ç¥¨
        for time_slot_candidates in aligned_candidates:
            if not time_slot_candidates:
                continue

            # æ”¶é›†æ‰€æœ‰å€™é€‰æ–‡æœ¬
            candidate_texts = [c['text'] for c in time_slot_candidates]
            candidate_confs = [c['confidence'] for c in time_slot_candidates]

            # ROVERæŠ•ç¥¨ï¼šé€‰æ‹©å‡ºç°é¢‘ç‡æœ€é«˜çš„æ–‡æœ¬
            text_counts = {}
            for text, conf in zip(candidate_texts, candidate_confs):
                if text not in text_counts:
                    text_counts[text] = {'count': 0, 'total_conf': 0.0}
                text_counts[text]['count'] += 1
                text_counts[text]['total_conf'] += conf

            # é€‰æ‹©æŠ•ç¥¨æ•°æœ€å¤šä¸”ç½®ä¿¡åº¦æœ€é«˜çš„æ–‡æœ¬
            best_text = max(text_counts.items(),
                          key=lambda x: (x[1]['count'], x[1]['total_conf']))[0]

            # è®¡ç®—èåˆç½®ä¿¡åº¦
            fused_conf = self.confidence_fuser.fuse_confidences(
                [c['confidence'] for c in time_slot_candidates if c['text'] == best_text]
            )

            fused_results.append({
                'text': best_text,
                'confidence': fused_conf,
                'votes': text_counts[best_text]['count'],
                'total_models': len(time_slot_candidates)
            })

        return fused_results

# ==================== æ—¶é—´å¯¹é½ç³»ç»Ÿ ====================

class TimeAlignmentSystem:
    """æ—¶é—´å¯¹é½ç³»ç»Ÿ - æ”¯æŒå¤šå€™é€‰èåˆ"""
    def __init__(self, tolerance=0.5):
        self.tolerance = tolerance  # æ—¶é—´å¯¹é½å®¹å¿åº¦(ç§’)

    def align_candidates(self, all_candidates):
        """å¯¹é½æ¥è‡ªä¸åŒæ¨¡å‹çš„å€™é€‰ç»“æœ"""
        if not all_candidates:
            return []

        # ç®€åŒ–çš„æ—¶é—´å¯¹é½å®ç°
        # å®é™…ROVERéœ€è¦å¤æ‚çš„DTW(dynamic time warping)ç®—æ³•

        aligned = []

        # å‡è®¾æ‰€æœ‰æ¨¡å‹å¤„ç†ç›¸åŒçš„éŸ³é¢‘ç‰‡æ®µ
        max_length = max(len(candidates) for candidates in all_candidates) if all_candidates else 0

        for i in range(max_length):
            time_slot = []
            for model_candidates in all_candidates:
                if i < len(model_candidates):
                    candidate = model_candidates[i].copy()
                    candidate['time_slot'] = i
                    time_slot.append(candidate)

            if time_slot:
                aligned.append(time_slot)

        return aligned

# ==================== ç½®ä¿¡åº¦èåˆå¼•æ“ ====================

class ConfidenceFusionEngine:
    """ç½®ä¿¡åº¦èåˆå¼•æ“ - åŸºäºç»Ÿè®¡æ¨¡å‹"""
    def __init__(self):
        self.fusion_method = 'weighted_average'  # æˆ– 'maximum', 'bayesian'

    def fuse_confidences(self, confidences):
        """èåˆå¤šä¸ªæ¨¡å‹çš„ç½®ä¿¡åº¦"""
        if not confidences:
            return 0.5

        if len(confidences) == 1:
            return confidences[0]

        if self.fusion_method == 'weighted_average':
            # åŠ æƒå¹³å‡ï¼Œè¶Šé«˜ç½®ä¿¡åº¦çš„æ¨¡å‹æƒé‡è¶Šå¤§
            weights = [c / sum(confidences) for c in confidences]
            fused = sum(c * w for c, w in zip(confidences, weights))

        elif self.fusion_method == 'maximum':
            fused = max(confidences)

        elif self.fusion_method == 'bayesian':
            # ç®€åŒ–çš„è´å¶æ–¯èåˆ
            fused = 1 - (1 - sum(confidences) / len(confidences)) ** 0.5

        else:
            fused = sum(confidences) / len(confidences)

        return min(1.0, max(0.0, fused))

# ==================== å¤šæ¨¡å‹ç®¡ç†å™¨ ====================

class MultiModelManager:
    """å¤šæ¨¡å‹ç®¡ç†å™¨ - æ”¯æŒROVERèåˆ"""
    def __init__(self):
        self.primary_model = None
        self.ensemble_models = []
        self.rover_system = None

    def load_multiple_models(self):
        """åŠ è½½å¤šä¸ªASRæ¨¡å‹ç”¨äºèåˆ"""
        models = []

        try:
            # æ¨¡å‹1: ç°æœ‰çš„Nanoæ¨¡å‹
            model1 = AutoModel(
                model="/root/autodl-tmp/Fun-ASR-Nano-2512",
                trust_remote_code=True,
                remote_code="/root/autodl-tmp/Fun-ASR-Nano-2512/model.py",
                device=DEVICE,
                batch_size=1,
            )
            models.append(model1)

            # æ¨¡å‹2: å¦‚æœå¯ç”¨ï¼ŒåŠ è½½å¦ä¸€ä¸ªå˜ä½“
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šæ¨¡å‹å˜ä½“
            # ä¾‹å¦‚: ä¸åŒçš„beam size, ä¸åŒçš„è¯­è¨€æ¨¡å‹æƒé‡ç­‰

        except Exception as e:
            print(f"Model loading failed: {e}")

        if len(models) >= 1:
            self.primary_model = models[0]
            self.ensemble_models = models[1:]
            self.rover_system = ROVERFusionSystem(models)

        return models

    def get_rover_fusion(self):
        """è·å–ROVERèåˆç³»ç»Ÿ"""
        return self.rover_system

# ==================== è¯´è¯äººè‡ªé€‚åº”ç³»ç»Ÿ ====================

class SpeakerAdaptationSystem:
    """è¯´è¯äººè‡ªé€‚åº”ç³»ç»Ÿ - ä¸ªæ€§åŒ–ä¼˜åŒ–"""
    def __init__(self, adaptation_rate=0.1):
        self.adaptation_rate = adaptation_rate
        self.speaker_profiles = {}
        self.speaker_stats = {}

    def adapt_to_speaker(self, speaker_name, text, confidence, audio_features=None):
        """æ ¹æ®è¯´è¯äººç‰¹å¾è¿›è¡Œè‡ªé€‚åº”"""
        if speaker_name not in self.speaker_profiles:
            self.speaker_profiles[speaker_name] = {
                'avg_confidence': 0.8,
                'text_patterns': {},
                'adaptation_count': 0,
                'audio_features': None
            }

        profile = self.speaker_profiles[speaker_name]

        # æ›´æ–°å¹³å‡ç½®ä¿¡åº¦
        old_avg = profile['avg_confidence']
        new_avg = (old_avg * profile['adaptation_count'] + confidence) / (profile['adaptation_count'] + 1)
        profile['avg_confidence'] = new_avg
        profile['adaptation_count'] += 1

        # å­¦ä¹ æ–‡æœ¬æ¨¡å¼
        self._learn_text_patterns(profile, text)

        # éŸ³é¢‘ç‰¹å¾è‡ªé€‚åº”
        if audio_features:
            self._adapt_audio_features(profile, audio_features)

    def _learn_text_patterns(self, profile, text):
        """å­¦ä¹ è¯´è¯äººçš„æ–‡æœ¬æ¨¡å¼"""
        if not text:
            return

        # ç®€å•çš„n-gramå­¦ä¹ 
        chars = list(text)
        for i in range(len(chars) - 1):
            bigram = ''.join(chars[i:i+2])
            if bigram not in profile['text_patterns']:
                profile['text_patterns'][bigram] = 0
            profile['text_patterns'][bigram] += 1

    def _adapt_audio_features(self, profile, audio_features):
        """è‡ªé€‚åº”éŸ³é¢‘ç‰¹å¾"""
        if profile['audio_features'] is None:
            profile['audio_features'] = audio_features
        else:
            # æŒ‡æ•°ç§»åŠ¨å¹³å‡
            profile['audio_features'] = (
                profile['audio_features'] * (1 - self.adaptation_rate) +
                audio_features * self.adaptation_rate
            )

    def get_adaptation_bonus(self, speaker_name, text, audio_features=None):
        """è·å–è‡ªé€‚åº”å¥–åŠ±åˆ†æ•°"""
        if speaker_name not in self.speaker_profiles:
            return 0.0

        profile = self.speaker_profiles[speaker_name]
        bonus = 0.0

        # ç½®ä¿¡åº¦å¥–åŠ±
        confidence_bonus = (profile['avg_confidence'] - 0.8) * 0.1
        bonus += confidence_bonus

        # æ–‡æœ¬æ¨¡å¼å¥–åŠ±
        pattern_bonus = self._compute_pattern_bonus(profile, text)
        bonus += pattern_bonus

        return bonus

    def _compute_pattern_bonus(self, profile, text):
        """è®¡ç®—æ–‡æœ¬æ¨¡å¼å¥–åŠ±"""
        if not text or not profile['text_patterns']:
            return 0.0

        chars = list(text)
        pattern_score = 0
        total_patterns = 0

        for i in range(len(chars) - 1):
            bigram = ''.join(chars[i:i+2])
            if bigram in profile['text_patterns']:
                pattern_score += profile['text_patterns'][bigram]
                total_patterns += 1

        if total_patterns > 0:
            return (pattern_score / total_patterns) * 0.05
        return 0.0

# ==================== è´¨é‡è¯„ä¼°ç³»ç»Ÿ ====================

class QualityEstimationSystem:
    """è´¨é‡è¯„ä¼°ç³»ç»Ÿ - å¤šç»´åº¦ç½®ä¿¡åº¦è¯„åˆ†"""
    def __init__(self):
        self.feature_weights = {
            'asr_confidence': 0.4,
            'lm_score': 0.2,
            'audio_quality': 0.15,
            'text_consistency': 0.15,
            'speaker_consistency': 0.1
        }

    def estimate_quality(self, segment, context=None, speaker_info=None, audio_features=None):
        """å¤šç»´åº¦è´¨é‡è¯„ä¼°"""
        scores = {}

        # ASRåŸå§‹ç½®ä¿¡åº¦
        scores['asr_confidence'] = segment.get('confidence', 0.5)

        # è¯­è¨€æ¨¡å‹è¯„åˆ†
        scores['lm_score'] = self._compute_lm_quality(segment.get('text', ''))

        # éŸ³é¢‘è´¨é‡è¯„åˆ†
        scores['audio_quality'] = self._compute_audio_quality(audio_features)

        # æ–‡æœ¬ä¸€è‡´æ€§è¯„åˆ†
        scores['text_consistency'] = self._compute_text_consistency(segment.get('text', ''), context)

        # è¯´è¯äººä¸€è‡´æ€§è¯„åˆ†
        scores['speaker_consistency'] = self._compute_speaker_consistency(speaker_info)

        # åŠ æƒèåˆ
        final_score = sum(scores[feature] * weight for feature, weight in self.feature_weights.items())

        return min(1.0, max(0.0, final_score)), scores

    def _compute_lm_quality(self, text):
        """è¯­è¨€æ¨¡å‹è´¨é‡è¯„åˆ†"""
        if not text:
            return 0.3

        score = 0.5

        # é•¿åº¦åˆç†æ€§
        if 3 <= len(text) <= 100:
            score += 0.2
        elif len(text) < 3:
            score -= 0.2

        # æ ‡ç‚¹ç¬¦å·
        if any(punct in text for punct in 'ã€‚ï¼ï¼Ÿï¼Œï¼›ï¼š'):
            score += 0.1

        # ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        chinese_ratio = chinese_chars / len(text) if text else 0
        if chinese_ratio > 0.7:
            score += 0.1
        elif chinese_ratio < 0.3:
            score -= 0.1

        return min(1.0, max(0.0, score))

    def _compute_audio_quality(self, audio_features):
        """éŸ³é¢‘è´¨é‡è¯„åˆ†"""
        if not audio_features:
            return 0.7

        score = 0.7

        # åŸºäºRMSçš„éŸ³é‡è¯„åˆ†
        rms = audio_features.get('rms', 0.1)
        if 0.05 < rms < 0.3:
            score += 0.1

        # ä¿¡å™ªæ¯”è¯„åˆ†
        snr = audio_features.get('snr', 10)
        if snr > 15:
            score += 0.1
        elif snr < 5:
            score -= 0.2

        return min(1.0, max(0.0, score))

    def _compute_text_consistency(self, text, context):
        """æ–‡æœ¬ä¸€è‡´æ€§è¯„åˆ†"""
        if not text or not context:
            return 0.7

        score = 0.7

        # ä¸ä¸Šä¸‹æ–‡çš„è¯­ä¹‰ç›¸ä¼¼åº¦
        context_text = ' '.join(context[-3:])  # æœ€è¿‘3ä¸ªç‰‡æ®µ
        similarity = self._compute_text_similarity(text, context_text)
        score += similarity * 0.2

        return min(1.0, max(0.0, score))

    def _compute_text_similarity(self, text1, text2):
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        if not text1 or not text2:
            return 0.0

        set1 = set(text1)
        set2 = set(text2)

        intersection = len(set1 & set2)
        union = len(set1 | set2)

        return intersection / union if union > 0 else 0.0

    def _compute_speaker_consistency(self, speaker_info):
        """è¯´è¯äººä¸€è‡´æ€§è¯„åˆ†"""
        if not speaker_info:
            return 0.8

        # ç®€å•çš„è¯´è¯äººä¸€è‡´æ€§æ£€æŸ¥
        return 0.8

# ==================== æ—¶åºå»ºæ¨¡ç³»ç»Ÿ ====================

class TemporalModelingSystem:
    """æ—¶åºå»ºæ¨¡ç³»ç»Ÿ - è€ƒè™‘æ—¶é—´ä¾èµ–å…³ç³»"""
    def __init__(self, temporal_weight=0.15):
        self.temporal_weight = temporal_weight
        self.segment_history = []
        self.transition_patterns = {}

    def model_temporal_dependencies(self, segments):
        """å»ºæ¨¡æ—¶åºä¾èµ–å…³ç³»"""
        if not segments:
            return segments

        enhanced_segments = []

        for i, segment in enumerate(segments):
            enhanced_segment = segment.copy()

            # è®¡ç®—æ—¶åºä¸€è‡´æ€§åˆ†æ•°
            temporal_score = self._compute_temporal_consistency(segment, i, segments)
            enhanced_segment['temporal_score'] = temporal_score

            # æ›´æ–°ç½®ä¿¡åº¦
            original_conf = segment.get('confidence', 0.5)
            enhanced_conf = original_conf * (1 - self.temporal_weight) + temporal_score * self.temporal_weight
            enhanced_segment['confidence'] = enhanced_conf

            enhanced_segments.append(enhanced_segment)

            # æ›´æ–°å†å²
            self.segment_history.append(segment)
            if len(self.segment_history) > 10:  # ä¿æŒæœ€è¿‘10ä¸ªç‰‡æ®µ
                self.segment_history.pop(0)

        return enhanced_segments

    def _compute_temporal_consistency(self, segment, index, all_segments):
        """è®¡ç®—æ—¶åºä¸€è‡´æ€§"""
        score = 0.7

        # æ£€æŸ¥ä¸å‰ä¸€ä¸ªç‰‡æ®µçš„æ—¶é—´é—´éš”
        if index > 0:
            prev_segment = all_segments[index - 1]
            time_gap = segment.get('start_time', 0) - prev_segment.get('end_time', 0)

            if 0.5 <= time_gap <= 3.0:
                score += 0.1
            elif time_gap > 5.0:
                score -= 0.1

        # æ£€æŸ¥è¯´è¯äººä¸€è‡´æ€§
        if index > 0:
            prev_speaker = all_segments[index - 1].get('speaker', '')
            current_speaker = segment.get('speaker', '')

            if prev_speaker == current_speaker:
                score += 0.05
            elif prev_speaker and current_speaker and prev_speaker != current_speaker:
                # è¯´è¯äººåˆ‡æ¢
                if time_gap > 0.5:  # æœ‰è¶³å¤Ÿçš„æ—¶é—´åˆ‡æ¢
                    score += 0.05

        return min(1.0, max(0.0, score))

# ==================== é¢†åŸŸè‡ªé€‚åº”ç³»ç»Ÿ ====================

class DomainAdaptationSystem:
    """é¢†åŸŸè‡ªé€‚åº”ç³»ç»Ÿ - é’ˆå¯¹ä¸åŒéŸ³é¢‘é¢†åŸŸä¼˜åŒ–"""
    def __init__(self):
        self.domain_profiles = {
            'meeting': {'keywords': ['ä¼šè®®', 'è®¨è®º', 'å†³å®š', 'é¡¹ç›®'], 'lm_weight': 0.35},
            'lecture': {'keywords': ['è¯¾ç¨‹', 'å­¦ä¹ ', 'çŸ¥è¯†', 'æ•™æˆ'], 'lm_weight': 0.4},
            'interview': {'keywords': ['é‡‡è®¿', 'é—®é¢˜', 'å›ç­”', 'è§‚ç‚¹'], 'lm_weight': 0.3},
            'conversation': {'keywords': ['èŠå¤©', 'æœ‹å‹', 'ç”Ÿæ´»', 'å·¥ä½œ'], 'lm_weight': 0.25},
            'news': {'keywords': ['æ–°é—»', 'æŠ¥é“', 'äº‹ä»¶', 'å‘ç”Ÿ'], 'lm_weight': 0.45},
        }

    def detect_domain(self, text_segments):
        """æ£€æµ‹éŸ³é¢‘é¢†åŸŸ"""
        if not text_segments:
            return 'general'

        all_text = ' '.join(text_segments)
        domain_scores = {}

        for domain, profile in self.domain_profiles.items():
            score = 0
            for keyword in profile['keywords']:
                if keyword in all_text:
                    score += 1
            domain_scores[domain] = score

        if domain_scores:
            best_domain = max(domain_scores, key=domain_scores.get)
            if domain_scores[best_domain] >= 2:
                return best_domain

        return 'general'

    def get_domain_adaptation(self, domain):
        """è·å–é¢†åŸŸè‡ªé€‚åº”å‚æ•°"""
        if domain in self.domain_profiles:
            return self.domain_profiles[domain]
        else:
            return {'lm_weight': 0.3}

# ==================== é«˜çº§æ–‡æœ¬åå¤„ç† ====================

class AdvancedTextPostProcessor:
    """é«˜çº§æ–‡æœ¬åå¤„ç† - å•†ä¸šçº§æ–‡æœ¬æ¸…ç†"""
    def __init__(self):
        self.error_patterns = self._load_error_patterns()
        self.correction_rules = self._load_correction_rules()

    def _load_error_patterns(self):
        """åŠ è½½å¸¸è§çš„é”™è¯¯æ¨¡å¼"""
        return {
            'é‡å¤è¯': r'(\w{2,})\1{2,}',  # ä¸‰ä¸ªæˆ–æ›´å¤šé‡å¤
            'è¿ç»­æ ‡ç‚¹': r'[ã€‚ï¼ï¼Ÿï¼Œï¼›ï¼š]{2,}',  # è¿ç»­æ ‡ç‚¹
            'å¼‚å¸¸ç©ºæ ¼': r'\s{2,}',  # å¤šä½™ç©ºæ ¼
        }

    def _load_correction_rules(self):
        """åŠ è½½çº é”™è§„åˆ™"""
        return {
            'çš„çš„': 'çš„',
            'äº†äº†': 'äº†',
            'æ˜¯æ˜¯': 'æ˜¯',
            'æœ‰æœ‰': 'æœ‰',
            'å’Œå’Œ': 'å’Œ',
            'ï¼Œï¼Œ': 'ï¼Œ',
            'ã€‚ã€‚': 'ã€‚',
            'ï¼ï¼': 'ï¼',
            'ï¼Ÿï¼Ÿ': 'ï¼Ÿ',
        }

    def post_process(self, text):
        """é«˜çº§æ–‡æœ¬åå¤„ç†"""
        if not text:
            return text

        # åº”ç”¨çº é”™è§„åˆ™
        for error, correction in self.correction_rules.items():
            text = text.replace(error, correction)

        # ç§»é™¤å¼‚å¸¸æ¨¡å¼
        for pattern_name, pattern in self.error_patterns.items():
            text = re.sub(pattern, '', text)

        # è§„èŒƒåŒ–æ ‡ç‚¹
        text = self._normalize_punctuation(text)

        # è§„èŒƒåŒ–ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def _normalize_punctuation(self, text):
        """è§„èŒƒåŒ–æ ‡ç‚¹ç¬¦å·"""
        # ç¡®ä¿å¥å­ç»“å°¾æœ‰é€‚å½“æ ‡ç‚¹
        if text and not text[-1] in 'ã€‚ï¼ï¼Ÿ':
            text += 'ã€‚'

        # ç§»é™¤è¿ç»­æ ‡ç‚¹
        text = re.sub(r'[ã€‚ï¼ï¼Ÿ]{2,}', 'ã€‚', text)
        text = re.sub(r'[ï¼Œï¼›ï¼š]{2,}', 'ï¼Œ', text)

        return text

# ==================== å•†ä¸šçº§è¯†åˆ«å¼•æ“ ====================

class CommercialGradeRecognitionEngine:
    """å•†ä¸šçº§è¯†åˆ«å¼•æ“ - è¾¾åˆ°ç§‘å¤§è®¯é£æ°´å¹³"""
    def __init__(self, asr_model, sv_model, vad_model, voiceprint_dir, **kwargs):
        self.asr_model = asr_model
        self.preprocessor = UltimateAudioPreprocessor()
        self.segmenter = IntelligentVADSegmenter(vad_model)
        self.emb_extractor = RobustEmbeddingExtractor(sv_model)
        self.sv_model = sv_model

        # å•†ä¸šçº§ç»„ä»¶
        self.context_lm = ContextAwareLanguageModel(lm_weight=0.4)
        self.rover_system = ROVERFusionSystem([asr_model])  # åˆå§‹åŒ–ROVERç³»ç»Ÿ
        self.speaker_adaptation = SpeakerAdaptationSystem()
        self.quality_estimator = QualityEstimationSystem()
        self.temporal_modeler = TemporalModelingSystem()
        self.domain_adapter = DomainAdaptationSystem()
        self.text_processor = AdvancedTextPostProcessor()

        self.repetition_suppressor = IndustrialRepetitionSuppressor()
        self.robust_decoder = RobustASRDecoder()
        self.adaptive_tuner = AdaptiveParameterTuner() if kwargs.get('enable_adaptive_tuning', True) else None

        self.registered_voices = self._load_voiceprints(voiceprint_dir)
        self.segment_history = []

    def _load_voiceprints(self, voiceprint_dir):
        voices = {}
        files = [f for f in os.listdir(voiceprint_dir) if f.endswith('.npy')]
        for file in files:
            name = os.path.splitext(file)[0]
            path = os.path.join(voiceprint_dir, file)
            emb = np.load(path)
            voices[name] = normalize_embedding(emb)
        return voices

    def process_audio(self, audio_path, progress_callback=None):
        speech, sr = sf.read(audio_path)
        if len(speech.shape) > 1:
            speech = speech.mean(axis=1)

        duration = len(speech) / sr

        if progress_callback:
            progress_callback(f"éŸ³é¢‘æ—¶é•¿: {duration:.1f}ç§’")

        # é¢†åŸŸæ£€æµ‹
        if progress_callback:
            progress_callback("æ£€æµ‹éŸ³é¢‘é¢†åŸŸ...")

        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥åŸºäºéŸ³é¢‘å†…å®¹æ£€æµ‹
        domain = 'general'
        domain_params = self.domain_adapter.get_domain_adaptation(domain)

        # è‡ªé€‚åº”å‚æ•°è°ƒæ•´
        adaptive_params = None
        if self.adaptive_tuner:
            if progress_callback:
                progress_callback("åˆ†æéŸ³é¢‘ç‰¹å¾...")

            audio_profile = self.adaptive_tuner.analyze_audio_profile(speech, sr)
            adaptive_params = self.adaptive_tuner.tune_parameters(audio_profile)

            if progress_callback:
                progress_callback(f"è‡ªé€‚åº”å‚æ•°: VAD={adaptive_params['vad_threshold']:.2f}, Beam={adaptive_params['beam_size']}")

        # é¢„å¤„ç†
        if progress_callback:
            progress_callback("å·¥ä¸šçº§é¢„å¤„ç†...")

        speech = self.preprocessor.preprocess(speech, sr, True)

        # åˆ†æ®µ
        if progress_callback:
            progress_callback("æ™ºèƒ½åˆ†æ®µ...")

        segments = self.segmenter.segment_with_vad(speech, sr)

        # æ—©æœŸé‡å¤æŠ‘åˆ¶ - å¯¹éŸ³é¢‘ç‰‡æ®µè¿›è¡Œé¢„å¤„ç†
        if progress_callback:
            progress_callback("éŸ³é¢‘çº§é‡å¤æŠ‘åˆ¶...")

        # è¿™é‡Œå¯ä»¥æ·»åŠ éŸ³é¢‘çº§åˆ«çš„é‡å¤æ£€æµ‹å’Œè¿‡æ»¤
        # ä¾‹å¦‚ï¼šæ£€æµ‹æ˜æ˜¾é‡å¤çš„éŸ³é¢‘æ¨¡å¼

        if progress_callback:
            progress_callback(f"åˆ†æ®µå®Œæˆ: {len(segments)} ä¸ªç‰‡æ®µ")

        # æ‰¹é‡å£°çº¹æå–
        if progress_callback:
            progress_callback("å£°çº¹è¯†åˆ«...")

        embeddings = []
        for idx, seg in enumerate(segments):
            if progress_callback and idx % 5 == 0:
                progress_callback(f"æå–å£°çº¹: {idx+1}/{len(segments)}")
            emb = self.emb_extractor.extract_embedding(seg['audio'], sr)
            embeddings.append(emb)

        vad_scores = [seg.get('vad_quality', 0.5) for seg in segments]

        # è¯´è¯äººè¯†åˆ«
        if progress_callback:
            progress_callback("è¯´è¯äººè¯†åˆ«...")

        speaker_names, speaker_confidences = self._identify_speakers(embeddings)

        # å•†ä¸šçº§è§£ç 
        results = []
        for idx, seg in enumerate(segments):
            if progress_callback and idx % 3 == 0:
                progress_callback(f"å•†ä¸šçº§è§£ç : {idx+1}/{len(segments)}")

            temp_path = os.path.join(TEMP_DIR, f"asr_{idx}_{int(time.time()*1000)}.wav")
            sf.write(temp_path, seg['audio'], sr)

            try:
                candidates = self.robust_decoder.decode_with_repetition_penalty(self.asr_model, temp_path, penalty=1.5)

                if candidates:
                    text, base_score = candidates[0]
                else:
                    text, base_score = "", 0.0

                if text:
                    # ä¸Šä¸‹æ–‡æ„ŸçŸ¥LMèåˆ
                    context = self.context_lm.get_context()
                    lm_fused_score = self.context_lm.fuse_scores(base_score, text, context)

                    # ROVERå¤šå€™é€‰èåˆ (å¦‚æœå¯ç”¨)
                    if hasattr(self, 'rover_system') and self.rover_system:
                        rover_results = self.rover_system.rover_fusion(temp_path)
                        if rover_results:
                            # ä½¿ç”¨ROVERç»“æœçš„æœ€é«˜ç½®ä¿¡åº¦
                            rover_best = max(rover_results, key=lambda x: x.get('confidence', 0))
                            final_score = rover_best.get('confidence', lm_fused_score)
                            text = rover_best.get('text', text)
                        else:
                            final_score = lm_fused_score
                    else:
                        final_score = lm_fused_score

                    # è¯´è¯äººè‡ªé€‚åº”
                    speaker_bonus = self.speaker_adaptation.get_adaptation_bonus(speaker_names[idx], text)
                    final_score += speaker_bonus

                    # è´¨é‡è¯„ä¼°
                    audio_features = {'rms': np.sqrt(np.mean(seg['audio']**2)), 'snr': 15}
                    quality_score, quality_details = self.quality_estimator.estimate_quality(
                        {'text': text, 'confidence': final_score},
                        context,
                        {'speaker': speaker_names[idx]},
                        audio_features
                    )

                    results.append({
                        'text': text,
                        'speaker': speaker_names[idx],
                        'confidence': quality_score,
                        'start_time': seg['start_time'],
                        'end_time': seg['end_time'],
                        'quality_details': quality_details
                    })

                    # è¯´è¯äººè‡ªé€‚åº”å­¦ä¹ 
                    self.speaker_adaptation.adapt_to_speaker(speaker_names[idx], text, quality_score, audio_features)

            except Exception as e:
                print(f"Decoding failed for segment {idx}: {e}")

            if os.path.exists(temp_path):
                os.remove(temp_path)

        # æ—¶åºå»ºæ¨¡
        if progress_callback:
            progress_callback("æ—¶åºä¼˜åŒ–...")

        results = self.temporal_modeler.model_temporal_dependencies(results)

        # ==================== è¶…çº§é‡å¤æŠ‘åˆ¶ç»ˆæé˜²å¾¡ ====================
        if progress_callback:
            progress_callback("æ·±åº¦æ–‡æœ¬å»é‡...")

        # æ–°å¢ï¼šä¸“æ€æ•´å¥é•¿é‡å¤ï¼ˆé’ˆå¯¹"èˆ†è®ºæ„è¯†"20è¿å‘è¿™ç±»ï¼‰
        def remove_long_sentence_repetitions(text, min_len=20):
            import re
            # æŒ‰ä¸­æ–‡æ ‡ç‚¹åˆ‡å¥
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]', text)
            sentences = [s.strip() for s in sentences if s.strip()]

            if len(sentences) <= 1:
                return text

            cleaned = []
            seen = set()

            for sent in sentences:
                if len(sent) < min_len:
                    cleaned.append(sent)
                    continue

                # å®Œå…¨ç›¸åŒé•¿å¥ç›´æ¥ä¸¢å¼ƒ
                if sent in seen:
                    continue
                seen.add(sent)
                cleaned.append(sent)

            result = 'ã€‚'.join(cleaned)
            if text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›')):
                result += 'ã€‚'
            return result

        # å¯¹æ‰€æœ‰æ®µè½è¿›è¡Œæ·±åº¦å»é‡
        for item in results:
            original = item['text']
            # ç¬¬ä¸€å±‚ï¼šæ•´å¥çº§å»é‡ï¼ˆæ€æ‰‹é”ï¼‰
            text = remove_long_sentence_repetitions(original)
            # ç¬¬äºŒå±‚ï¼šè°ƒç”¨å·¥ä¸šçº§å»é‡å™¨
            text = self.repetition_suppressor.aggressive_dedup(text)
            # ç¬¬ä¸‰å±‚ï¼šå†æ¬¡ä¿é™©å»é‡ï¼ˆçŸ­å¥æ¨¡ç³Šç›¸ä¼¼ï¼‰
            text = self.repetition_suppressor._remove_semantic_repetitions(text)

            item['text'] = text.strip()

        # å…¨å±€å†å»ä¸€æ¬¡è·¨æ®µè½é‡å¤ï¼ˆé˜²æ­¢ä¸åŒæ®µè½é‡å¤åŒä¸€å¥ï¼‰
        all_texts = [r['text'] for r in results if len(r['text']) > 10]
        unique_texts = []
        seen_global = set()
        new_results = []

        for r in results:
            text = r['text']
            if len(text) > 10 and text in seen_global:
                continue  # è·¨æ®µè½å®Œå…¨é‡å¤ç›´æ¥ä¸¢
            if len(text) > 10:
                seen_global.add(text)
            new_results.append(r)

        results = new_results

        # æƒ©ç½šæ˜æ˜¾é‡å¤ç”Ÿæˆçš„æ®µè½
        for r in results:
            text = r['text']
            if len(text) > 50:
                # è®¡ç®—å†…éƒ¨é‡å¤ç‡
                words = list(text)
                if len(words) > 10:
                    from collections import Counter
                    counter = Counter(words)
                    repeat_ratio = sum(count ** 2 for count in counter.values()) / len(words) ** 2
                    if repeat_ratio > 0.1:  # é«˜åº¦é‡å¤
                        r['confidence'] *= 0.3

        # é«˜çº§æ–‡æœ¬åå¤„ç†
        if progress_callback:
            progress_callback("æ–‡æœ¬åå¤„ç†...")

        for result in results:
            result['text'] = self.text_processor.post_process(result['text'])

        # æ ‡ç‚¹æ¢å¤
        results = ContextualPunctuationRestorer.restore(results)

        # åˆå¹¶ç»“æœ
        if progress_callback:
            progress_callback("ç»“æœåˆå¹¶...")

        return self._merge_results(results)

    def _identify_speakers(self, embeddings):
        names = []
        confidences = []

        for emb in embeddings:
            if emb is None or not self.registered_voices:
                names.append("è¯´è¯äºº")
                confidences.append(0.0)
                continue

            emb_norm = normalize_embedding(emb)
            best_score = 0.0
            best_name = "è¯´è¯äºº"

            for name, ref_emb in self.registered_voices.items():
                score = np.dot(emb_norm, ref_emb)
                if score > best_score:
                    best_score = score
                    best_name = name

            if best_score >= 0.65:
                names.append(best_name)
                confidences.append(best_score)
            else:
                names.append("è¯´è¯äºº")
                confidences.append(0.0)

        return names, confidences

    def _merge_results(self, results):
        if not results:
            return []

        merged = []
        current = results[0].copy()

        for i in range(1, len(results)):
            next_item = results[i]

            if (current['speaker'] == next_item['speaker'] and
                next_item['start_time'] - current['end_time'] < 2.0):

                if current['text'] and not current['text'][-1] in 'ã€‚!?':
                    current['text'] += ','

                current['text'] += next_item['text']
                current['end_time'] = next_item['end_time']
                current['confidence'] = (current['confidence'] + next_item['confidence']) / 2
            else:
                merged.append(current)
                current = next_item.copy()

        merged.append(current)
        return merged

# ==================== Beam Searchè§£ç å™¨ ====================

class BeamSearchDecoder:
    """Beam SearchæŸæœç´¢è§£ç å™¨"""
    
    def __init__(self, beam_size=5, length_penalty=0.6):
        self.beam_size = beam_size
        self.length_penalty = length_penalty
    
    def decode(self, hypotheses):
        if not hypotheses:
            return "", 0.0
        
        scored_hyps = []
        for text, score in hypotheses:
            length = max(1, len(text))
            normalized_score = score / (length ** self.length_penalty)
            scored_hyps.append((normalized_score, text, score))
        
        scored_hyps.sort(reverse=True, key=lambda x: x[0])
        
        if scored_hyps:
            return scored_hyps[0][1], scored_hyps[0][2]
        
        return "", 0.0

# ==================== ç½®ä¿¡åº¦æ ¡å‡†å™¨ ====================

class ConfidenceCalibrator:
    """Temperature Scalingç½®ä¿¡åº¦æ ¡å‡†"""
    
    def __init__(self, temperature=1.5):
        self.temperature = temperature
    
    def calibrate(self, raw_confidence, audio_quality=0.8):
        calibrated = raw_confidence ** (1 / self.temperature)
        calibrated = calibrated * (0.7 + 0.3 * audio_quality)
        return min(1.0, calibrated)

# ==================== ä¸¤é˜¶æ®µè§£ç å™¨ ====================

class TwoStageDecoder:
    """ä¸¤é˜¶æ®µè§£ç ç­–ç•¥"""
    
    def __init__(self, lm_fusion):
        self.lm_fusion = lm_fusion
        self.coarse_beam = 3
        self.fine_beam = 10
    
    def decode_coarse(self, asr_results):
        if not asr_results:
            return []
        return asr_results[:self.coarse_beam]
    
    def decode_fine(self, coarse_results):
        rescored = []
        
        for text, asr_score in coarse_results:
            fused_score = self.lm_fusion.fuse_scores(asr_score, text)
            rescored.append((text, fused_score))
        
        rescored.sort(key=lambda x: x[1], reverse=True)
        
        if rescored:
            return rescored[0]
        
        return "", 0.0

# ==================== éŸ³é¢‘é¢„å¤„ç†å™¨ ====================

class UltimateAudioPreprocessor:
    """ç»ˆæéŸ³é¢‘é¢„å¤„ç†"""
    
    @staticmethod
    def preprocess(audio, sr, enable_denoise=True):
        audio = UltimateAudioPreprocessor.normalize_volume(audio)
        
        if enable_denoise and noisereduce is not None:
            try:
                audio = noisereduce.reduce_noise(
                    y=audio, sr=sr, stationary=True, prop_decrease=0.85
                )
            except:
                pass
        
        audio = UltimateAudioPreprocessor.high_pass_filter(audio, sr, cutoff=80)
        audio = UltimateAudioPreprocessor.compress_dynamic_range(audio)
        
        return audio
    
    @staticmethod
    def normalize_volume(audio, target_db=-20):
        rms = np.sqrt(np.mean(audio**2))
        if rms < 1e-8:
            return audio
        
        current_db = 20 * np.log10(rms)
        gain = 10 ** ((target_db - current_db) / 20)
        gain = min(gain, 10.0)
        
        return audio * gain
    
    @staticmethod
    def high_pass_filter(audio, sr, cutoff=80):
        from scipy.signal import butter, filtfilt
        
        nyquist = sr / 2
        normal_cutoff = cutoff / nyquist
        b, a = butter(4, normal_cutoff, btype='high', analog=False)
        
        return filtfilt(b, a, audio)
    
    @staticmethod
    def compress_dynamic_range(audio, threshold=0.3, ratio=4.0):
        compressed = np.copy(audio)
        mask = np.abs(audio) > threshold
        compressed[mask] = np.sign(audio[mask]) * (
            threshold + (np.abs(audio[mask]) - threshold) / ratio
        )
        return compressed

# ==================== VADåˆ†æ®µå™¨ ====================

class IntelligentVADSegmenter:
    """æ™ºèƒ½VADåˆ†æ®µ"""
    
    def __init__(self, vad_model):
        self.vad_model = vad_model
    
    def segment_with_vad(self, speech, sr, max_duration=30, min_duration=1.5):
        segments = []
        
        try:
            temp_path = os.path.join(TEMP_DIR, f"temp_vad_{int(time.time()*1000)}.wav")
            sf.write(temp_path, speech, sr)
            
            vad_result = self.vad_model.generate(
                input=temp_path, max_single_segment_time=max_duration * 1000
            )
            
            if vad_result and len(vad_result) > 0:
                vad_segments = vad_result[0].get('value', []) if isinstance(vad_result[0], dict) else []
                merged_segments = self._merge_close_segments(vad_segments, gap_threshold=400)
                
                for seg in merged_segments:
                    start_ms, end_ms = seg[0], seg[1]
                    duration_ms = end_ms - start_ms
                    
                    if duration_ms < min_duration * 1000:
                        continue
                    
                    start_ms = max(0, start_ms - 150)
                    end_ms = min(len(speech) / sr * 1000, end_ms + 150)
                    
                    start_sample = int(start_ms * sr / 1000)
                    end_sample = int(end_ms * sr / 1000)
                    
                    segment_audio = speech[start_sample:end_sample]
                    vad_quality = self._compute_segment_quality(segment_audio, sr)
                    
                    segments.append({
                        'audio': segment_audio,
                        'start_time': start_ms / 1000,
                        'end_time': end_ms / 1000,
                        'duration': duration_ms / 1000,
                        'vad_quality': vad_quality
                    })
            
            if os.path.exists(temp_path):
                os.remove(temp_path)
            
            if len(segments) == 0:
                segments = self._fallback_segmentation(speech, sr, max_duration)
        
        except:
            segments = self._fallback_segmentation(speech, sr, max_duration)
        
        return segments
    
    def _merge_close_segments(self, segments, gap_threshold=400):
        if not segments:
            return []
        
        merged = [segments[0]]
        for seg in segments[1:]:
            prev_end = merged[-1][1]
            curr_start = seg[0]
            
            if curr_start - prev_end < gap_threshold:
                merged[-1] = [merged[-1][0], seg[1]]
            else:
                merged.append(seg)
        
        return merged
    
    def _compute_segment_quality(self, audio, sr):
        rms = np.sqrt(np.mean(audio**2))
        snr_score = min(1.0, rms * 10)
        return snr_score
    
    def _fallback_segmentation(self, speech, sr, chunk_duration=20):
        segments = []
        chunk_samples = int(chunk_duration * sr)
        overlap_samples = int(2 * sr)
        
        for i in range(0, len(speech), chunk_samples - overlap_samples):
            end = min(i + chunk_samples, len(speech))
            segment_audio = speech[i:end]
            
            segments.append({
                'audio': segment_audio,
                'start_time': i / sr,
                'end_time': end / sr,
                'duration': (end - i) / sr,
                'vad_quality': 0.5
            })
            if end == len(speech):
                break
        
        return segments

# ==================== å£°çº¹æå–å™¨ ====================

class RobustEmbeddingExtractor:
    """é²æ£’å£°çº¹æå–"""
    
    def __init__(self, sv_model):
        self.sv_model = sv_model
    
    def extract_embedding(self, audio, sr):
        temp_path = os.path.join(TEMP_DIR, f"emb_{int(time.time()*1000)}.wav")
        sf.write(temp_path, audio, sr)
        
        try:
            res = self.sv_model.generate(input=temp_path)
            if res and isinstance(res, list) and len(res) > 0:
                item = res[0]
                if isinstance(item, dict):
                    for key in ["embedding", "spk_embedding", "emb"]:
                        if key in item:
                            emb = tensor_to_numpy(item[key])
                            if os.path.exists(temp_path):
                                os.remove(temp_path)
                            return emb
                elif hasattr(item, 'embedding'):
                    emb = tensor_to_numpy(item.embedding)
                    if os.path.exists(temp_path):
                        os.remove(temp_path)
                    return emb
        except:
            pass
        
        if os.path.exists(temp_path):
            os.remove(temp_path)
        
        return None

# ==================== è¯´è¯äººèšç±» ====================

class SpeakerDiarizationEngine:
    """è¯´è¯äººæ—¥å¿—"""
    
    def __init__(self, min_speakers=2, max_speakers=10):
        self.min_speakers = min_speakers
        self.max_speakers = max_speakers
    
    def cluster_speakers(self, embeddings):
        if not embeddings or len(embeddings) < 2:
            return [0] * len(embeddings)
        
        valid_embeddings = []
        valid_indices = []
        
        for i, emb in enumerate(embeddings):
            if emb is not None:
                valid_embeddings.append(emb)
                valid_indices.append(i)
        
        if len(valid_embeddings) < 2:
            return [0] * len(embeddings)
        
        X = np.array([e.flatten() for e in valid_embeddings])
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        try:
            clusterer = HDBSCAN(
                min_cluster_size=max(2, len(X) // 8),
                min_samples=1,
                metric='euclidean'
            )
            labels = clusterer.fit_predict(X_scaled)
        except:
            from sklearn.cluster import AgglomerativeClustering
            n_clusters = min(self.max_speakers, max(self.min_speakers, len(X) // 5))
            clusterer = AgglomerativeClustering(n_clusters=n_clusters)
            labels = clusterer.fit_predict(X_scaled)
        
        full_labels = [-1] * len(embeddings)
        for i, idx in enumerate(valid_indices):
            full_labels[idx] = labels[i]
        
        full_labels = self._assign_noise_points(embeddings, full_labels)
        return full_labels
    
    def _assign_noise_points(self, embeddings, labels):
        for i, label in enumerate(labels):
            if label == -1 and embeddings[i] is not None:
                min_dist = float('inf')
                best_label = 0
                
                for j, other_label in enumerate(labels):
                    if other_label != -1 and embeddings[j] is not None:
                        dist = np.linalg.norm(embeddings[i] - embeddings[j])
                        if dist < min_dist:
                            min_dist = dist
                            best_label = other_label
                
                labels[i] = best_label
        
        return labels
    
    def generate_speaker_names(self, labels):
        unique_labels = sorted(set(l for l in labels if l != -1))
        label_to_name = {}
        
        for i, label in enumerate(unique_labels):
            label_to_name[label] = f"è¯´è¯äºº{chr(65 + i)}"
        
        label_to_name[-1] = "æœªçŸ¥è¯´è¯äºº"
        return [label_to_name[l] for l in labels]

# ==================== æ ‡ç‚¹æ¢å¤å™¨ ====================

class ContextualPunctuationRestorer:
    """ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ ‡ç‚¹æ¢å¤"""
    
    QUESTION_MARKERS = ['å—', 'å‘¢', 'å•Š', 'å“‡', 'ä¹ˆ', 'å˜›', 'å§']
    EXCLAMATION_MARKERS = ['å•Š', 'å“', 'å“‡', 'å‘€', 'å“Ÿ', 'å˜', 'å‘¦']
    
    @staticmethod
    def restore(segments):
        if not segments:
            return segments
        
        for i, seg in enumerate(segments):
            text = seg['text']
            
            if not text or len(text) < 2:
                continue
            
            if text[-1] in 'ã€‚!?,;:':
                continue
            
            pause = 0.0
            if i < len(segments) - 1:
                pause = segments[i + 1]['start_time'] - seg['end_time']
            
            punctuation = ContextualPunctuationRestorer._detect_by_tone(text)
            
            if not punctuation:
                if pause > 1.5:
                    punctuation = 'ã€‚'
                elif pause > 0.8:
                    punctuation = ','
                elif i == len(segments) - 1:
                    punctuation = 'ã€‚'
            
            if punctuation:
                seg['text'] = text + punctuation
        
        return segments
    
    @staticmethod
    def _detect_by_tone(text):
        last_char = text[-1]
        
        if last_char in ContextualPunctuationRestorer.QUESTION_MARKERS:
            return '?'
        
        if last_char in ContextualPunctuationRestorer.EXCLAMATION_MARKERS:
            return '!'
        
        ending_pattern = r'(çš„|äº†|è¿‡|ç€|æ˜¯)$'
        if re.search(ending_pattern, text):
            return 'ã€‚'
        
        return None

# ==================== é«˜çº§æ¨¡å‹ç®¡ç†å™¨ ====================

class AdvancedModelManager:
    """é«˜çº§æ¨¡å‹ç®¡ç†å™¨ - æ”¯æŒå¤šç§æ¶æ„é€‰æ‹©"""
    def __init__(self):
        self._asr = None
        self._sv = None
        self._vad = None
        self._nsd = None
        self._punc = None
        self._sensevoice = None
        self.model_architecture = "funasr_nano"  # é»˜è®¤æ¶æ„
        self._loading_timeout = 120  # å¢åŠ åˆ°120ç§’è¶…æ—¶

    def select_architecture(self, architecture):
        """é€‰æ‹©æ¨¡å‹æ¶æ„"""
        self.model_architecture = architecture

    def load_models(self):
        """åŠ è½½æ¨¡å‹ï¼Œç®€åŒ–ç‰ˆä»¥ç¡®ä¿å¯é æ€§"""
        if self._asr is not None:
            return self._asr, self._sv, self._vad, self._nsd, self._punc, self._sensevoice

        try:
            # å¼ºåˆ¶ä½¿ç”¨æœ€ç¨³å®šçš„FunASR Nanoé…ç½®
            st.info("ä½¿ç”¨ç¨³å®šé…ç½®åŠ è½½æ¨¡å‹...")
            return self._load_minimal_funasr_stack()

        except Exception as e:
            st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            st.info("å°è¯•æœ€å°åŒ–é…ç½®...")
            return self._load_minimal_fallback()

    def _load_sensevoice_stack(self):
        """åŠ è½½SenseVoiceå¤šåŠŸèƒ½ç»Ÿä¸€æ¨¡å‹æ ˆ"""
        with st.spinner("åŠ è½½SenseVoiceå¤šåŠŸèƒ½ç»Ÿä¸€æ¨¡å‹..."):
            try:
                # æ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼Œé˜²æ­¢åŠ è½½å¡ä½
                import signal
                import time

                def timeout_handler(signum, frame):
                    raise TimeoutError("æ¨¡å‹åŠ è½½è¶…æ—¶")

                # è®¾ç½®30ç§’è¶…æ—¶
                old_handler = signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(30)

                try:
                    # SenseVoice-Small: å¤šè¯­è¨€+æ ‡ç‚¹+æƒ…æ„Ÿ+äº‹ä»¶æ£€æµ‹+è¯´è¯äººåˆ¤æ–­
                    self._sensevoice = AutoModel(
                        model="iic/SenseVoiceSmall",
                        vad_model="fsmn-vad",
                        punc_model="ct-transformer_zh-cn-common-vocab272727-pytorch",
                        device=DEVICE,
                        disable_update=True,
                    )
                    self._asr = self._sensevoice  # SenseVoiceä½œä¸ºä¸»è¦ASRæ¨¡å‹
                    st.info("ğŸ‰ SenseVoiceå·²åŠ è½½: æ”¯æŒä¸­è‹±æ—¥éŸ©ç²¤+æƒ…æ„Ÿ+äº‹ä»¶æ£€æµ‹")
                finally:
                    signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
                    signal.signal(signal.SIGALRM, old_handler)

            except TimeoutError:
                st.error("SenseVoiceæ¨¡å‹åŠ è½½è¶…æ—¶ï¼Œå›é€€åˆ°FunASR")
                self.model_architecture = "funasr_nano"
                return self._load_funasr_stack()
            except Exception as e:
                st.warning(f"SenseVoiceåŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°FunASR: {e}")
                self.model_architecture = "funasr_nano"
                return self._load_funasr_stack()

        # SenseVoiceå·²ç»é›†æˆäº†VADå’Œæ ‡ç‚¹ï¼Œä½†ä¿ç•™ç‹¬ç«‹çš„å£°çº¹æ¨¡å‹
        with st.spinner("åŠ è½½å£°çº¹æ¨¡å‹..."):
            try:
                self._sv = AutoModel(
                    model="iic/speech_campplus_sv_zh-cn_16k-common",
                    device=DEVICE,
                    disable_update=True,
                )
            except Exception as e:
                print(f"Speaker verification model loading failed: {e}")
                self._sv = None

        st.success("SenseVoiceå¤šåŠŸèƒ½æ¨¡å‹æ ˆåŠ è½½å®Œæˆ!")
        return self._asr, self._sv, self._vad, self._nsd, self._punc, self._sensevoice

    def _load_funasr_stack(self):
        """åŠ è½½FunASRä¼ ç»Ÿæ¨¡å‹æ ˆ"""
        # ä½¿ç”¨é«˜ç²¾åº¦ASRæ¨¡å‹ - FunASR Nanoä½œä¸ºåŸºç¡€ï¼Œé…åˆä¸“ç”¨æ ‡ç‚¹æ¨¡å‹
        with st.spinner("åŠ è½½é«˜ç²¾åº¦ASRæ¨¡å‹..."):
            self._asr = AutoModel(
                model="/root/autodl-tmp/Fun-ASR-Nano-2512",
                trust_remote_code=True,
                remote_code="/root/autodl-tmp/Fun-ASR-Nano-2512/model.py",
                device=DEVICE,
                batch_size=1,
            )

        # å£°çº¹æ¨¡å‹ä»ç„¶ä½¿ç”¨ç‹¬ç«‹çš„
        with st.spinner("åŠ è½½å£°çº¹æ¨¡å‹..."):
            self._sv = AutoModel(
                model="iic/speech_campplus_sv_zh-cn_16k-common",
                device=DEVICE,
                disable_update=True,
            )

        # VADæ¨¡å‹å·²é›†æˆåˆ°ASRä¸­ï¼Œä½†ä¿ç•™ç‹¬ç«‹çš„ç”¨äºç‰¹æ®Šå¤„ç†
        with st.spinner("åŠ è½½VADæ¨¡å‹..."):
            self._vad = AutoModel(
                model="iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
                device=DEVICE,
                disable_update=True,
            )

        # ç¥ç»è¯´è¯äººåˆ†å‰²æ¨¡å‹ - æ›¿ä»£ä¼ ç»Ÿèšç±» (å¯é€‰)
        with st.spinner("åŠ è½½ç¥ç»è¯´è¯äººåˆ†å‰²æ¨¡å‹..."):
            try:
                self._nsd = AutoModel(
                    model="speech_microsoft-nsd_asr_nat-zh-cn-16k-common-vocab544-pytorch",
                    device=DEVICE,
                    disable_update=True,
                )
            except Exception as e:
                print(f"NSD model loading failed: {e}")
                self._nsd = None

        # ç‹¬ç«‹çš„æ ‡ç‚¹æ¢å¤æ¨¡å‹ä½œä¸ºåå¤‡ (å¯é€‰)
        with st.spinner("åŠ è½½æ ‡ç‚¹æ¢å¤æ¨¡å‹..."):
            try:
                self._punc = AutoModel(
                    model="speech_ct-transformer_punc_nat-zh-cn-16k-common-vocab272727-pytorch",
                    device=DEVICE,
                    disable_update=True,
                )
            except Exception as e:
                print(f"Punctuation model loading failed: {e}")
                self._punc = None

        st.success("FunASRæ¨¡å‹æ ˆåŠ è½½å®Œæˆ!")
        return self._asr, self._sv, self._vad, self._nsd, self._punc, self._sensevoice

    def _load_minimal_funasr_stack(self):
        """åŠ è½½æœ€å°åŒ–FunASRé…ç½® - æœ€ç¨³å®šç‰ˆæœ¬"""
        try:
            # åªåŠ è½½æ ¸å¿ƒASRæ¨¡å‹
            with st.spinner("åŠ è½½åŸºç¡€ASRæ¨¡å‹..."):
                self._asr = AutoModel(
                    model="/root/autodl-tmp/Fun-ASR-Nano-2512",
                    trust_remote_code=True,
                    remote_code="/root/autodl-tmp/Fun-ASR-Nano-2512/model.py",
                    device=DEVICE,
                    batch_size=1,
                )

            # å¯é€‰ç»„ä»¶ï¼Œå¦‚æœåŠ è½½å¤±è´¥åˆ™è·³è¿‡
            try:
                with st.spinner("åŠ è½½å£°çº¹æ¨¡å‹..."):
                    self._sv = AutoModel(
                        model="iic/speech_campplus_sv_zh-cn_16k-common",
                        device=DEVICE,
                        disable_update=True,
                    )
            except:
                st.warning("å£°çº¹æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å¼")
                self._sv = None

            try:
                with st.spinner("åŠ è½½VADæ¨¡å‹..."):
                    self._vad = AutoModel(
                        model="iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
                        device=DEVICE,
                        disable_update=True,
                    )
            except:
                st.warning("VADæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å¼")
                self._vad = None

            # é«˜çº§æ¨¡å‹è®¾ä¸ºNone
            self._nsd = None
            self._punc = None
            self._sensevoice = None

            st.success("åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ!")
            return self._asr, self._sv, self._vad, self._nsd, self._punc, self._sensevoice

        except Exception as e:
            st.error(f"åŸºç¡€æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def _load_minimal_fallback(self):
        """æœ€å°åŒ–fallback - åªæœ‰åŸºç¡€åŠŸèƒ½"""
        st.warning("ä½¿ç”¨æœ€å°åŒ–é…ç½®ï¼Œå¯èƒ½åŠŸèƒ½å—é™")

        # å°è¯•æœ€åŸºæœ¬çš„è®¾ç½®
        try:
            self._asr = AutoModel(
                model="/root/autodl-tmp/Fun-ASR-Nano-2512",
                trust_remote_code=True,
                device=DEVICE,
            )
        except Exception as e:
            st.error(f"æ— æ³•åŠ è½½ä»»ä½•æ¨¡å‹: {e}")
            self._asr = None

        # å…¶ä»–ç»„ä»¶è®¾ä¸ºNone
        self._sv = None
        self._vad = None
        self._nsd = None
        self._punc = None
        self._sensevoice = None

        if self._asr:
            st.success("æœ€å°åŒ–æ¨¡å¼åŠ è½½å®Œæˆ")
        else:
            st.error("æ— æ³•åŠ è½½ä»»ä½•æ¨¡å‹ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")

        return self._asr, self._sv, self._vad, self._nsd, self._punc, self._sensevoice

    def get_model_info(self):
        """è·å–å½“å‰æ¨¡å‹ä¿¡æ¯"""
        info = {
            "architecture": self.model_architecture,
            "asr_model": "SenseVoice-Small" if self.model_architecture == "sensevoice" else "FunASR-Nano",
            "multilingual": self.model_architecture == "sensevoice",
            "emotion_detection": self.model_architecture == "sensevoice",
            "event_detection": self.model_architecture == "sensevoice",
            "auto_speaker_count": self.model_architecture == "sensevoice",
        }
        return info

# å‘åå…¼å®¹çš„ModelManagerç±»
class ModelManager(AdvancedModelManager):
    """å‘åå…¼å®¹çš„æ¨¡å‹ç®¡ç†å™¨"""
    def __init__(self):
        super().__init__()
        # é»˜è®¤ä½¿ç”¨FunASR Nanoä»¥ä¿æŒå…¼å®¹æ€§
        self.model_architecture = "funasr_nano"

# ==================== è¯†åˆ«å¼•æ“ ====================

class UltimateRecognitionEngine:
    def __init__(self, asr_model, sv_model, vad_model, voiceprint_dir, 
                 enable_denoise=True, enable_diarization=True, 
                 enable_lm_fusion=True, enable_beam_search=True,
                 enable_batch_processing=True, enable_adaptive_tuning=True):
        self.asr_model = asr_model
        self.preprocessor = UltimateAudioPreprocessor()
        self.segmenter = IntelligentVADSegmenter(vad_model)
        self.emb_extractor = RobustEmbeddingExtractor(sv_model)
        self.sv_model = sv_model
        
        self.diarization = SpeakerDiarizationEngine() if enable_diarization else None
        self.lm_fusion = LanguageModelFusion() if enable_lm_fusion else None
        self.beam_decoder = BeamSearchDecoder() if enable_beam_search else None
        self.confidence_calibrator = ConfidenceCalibrator()
        self.two_stage_decoder = TwoStageDecoder(self.lm_fusion) if enable_lm_fusion else None
        self.punctuation = ContextualPunctuationRestorer()
        
        self.repetition_suppressor = IndustrialRepetitionSuppressor()
        self.robust_decoder = RobustASRDecoder()
        self.adaptive_tuner = AdaptiveParameterTuner() if enable_adaptive_tuning else None
        
        self.enable_denoise = enable_denoise
        self.enable_diarization = enable_diarization
        self.enable_lm_fusion = enable_lm_fusion
        self.enable_beam_search = enable_beam_search
        self.enable_batch_processing = enable_batch_processing
        self.enable_adaptive_tuning = enable_adaptive_tuning
        
        self.registered_voices = self._load_voiceprints(voiceprint_dir)
    
    def _load_voiceprints(self, voiceprint_dir):
        voices = {}
        files = [f for f in os.listdir(voiceprint_dir) if f.endswith('.npy')]
        for file in files:
            name = os.path.splitext(file)[0]
            path = os.path.join(voiceprint_dir, file)
            emb = np.load(path)
            voices[name] = normalize_embedding(emb)
        return voices
    
    def process_audio(self, audio_path, progress_callback=None):
        speech, sr = sf.read(audio_path)
        if len(speech.shape) > 1:
            speech = speech.mean(axis=1)
        
        duration = len(speech) / sr
        
        if progress_callback:
            progress_callback(f"éŸ³é¢‘æ—¶é•¿: {duration:.1f}ç§’")
        
        adaptive_params = None
        if self.enable_adaptive_tuning and self.adaptive_tuner:
            if progress_callback:
                progress_callback("åˆ†æéŸ³é¢‘ç‰¹å¾...")
            
            audio_profile = self.adaptive_tuner.analyze_audio_profile(speech, sr)
            adaptive_params = self.adaptive_tuner.tune_parameters(audio_profile)
            
            if progress_callback:
                progress_callback(f"è‡ªé€‚åº”å‚æ•°: VADé˜ˆå€¼={adaptive_params['vad_threshold']:.2f}, Beam={adaptive_params['beam_size']}")
        
        if progress_callback:
            progress_callback("å·¥ä¸šçº§é¢„å¤„ç†...")
        
        speech = self.preprocessor.preprocess(speech, sr, self.enable_denoise)
        
        if progress_callback:
            progress_callback("è‡ªé€‚åº”åˆ†æ®µ...")
        
        segments = self.segmenter.segment_with_vad(speech, sr)
        
        if progress_callback:
            progress_callback(f"åˆ†æ®µå®Œæˆ: {len(segments)} ä¸ªç‰‡æ®µ")
        
        if progress_callback:
            progress_callback("æ‰¹é‡æå–å£°çº¹...")
        
        if self.enable_batch_processing and len(segments) > 3:
            embeddings = BatchProcessor.batch_extract_embeddings(segments, sr, self.sv_model, batch_size=5)
        else:
            embeddings = []
            for idx, seg in enumerate(segments):
                if progress_callback and idx % 5 == 0:
                    progress_callback(f"æå–å£°çº¹: {idx+1}/{len(segments)}")
                emb = self.emb_extractor.extract_embedding(seg['audio'], sr)
                embeddings.append(emb)
        
        vad_scores = [seg.get('vad_quality', 0.5) for seg in segments]
        
        if progress_callback:
            progress_callback("è¯†åˆ«è¯´è¯äºº...")
        
        if self.enable_diarization and not self.registered_voices:
            speaker_labels = self.diarization.cluster_speakers(embeddings)
            speaker_names = self.diarization.generate_speaker_names(speaker_labels)
            speaker_confidences = [0.8] * len(speaker_names)
        else:
            speaker_names, speaker_confidences = self._match_speakers(embeddings)
        
        results = []
        for idx, seg in enumerate(segments):
            if progress_callback and idx % 3 == 0:
                progress_callback(f"é²æ£’è§£ç : {idx+1}/{len(segments)}")
            
            temp_path = os.path.join(TEMP_DIR, f"asr_{idx}_{int(time.time()*1000)}.wav")
            sf.write(temp_path, seg['audio'], sr)
            
            try:
                candidates = self.robust_decoder.decode_with_repetition_penalty(
                    self.asr_model, temp_path, penalty=1.5
                )
                
                if candidates and self.two_stage_decoder:
                    text, final_score = self.two_stage_decoder.decode_fine(
                        self.two_stage_decoder.decode_coarse(candidates)
                    )
                elif candidates:
                    text, final_score = candidates[0]
                else:
                    text, final_score = "", 0.0
                
                if text:
                    calibrated_conf = self.confidence_calibrator.calibrate(
                        final_score, vad_scores[idx]
                    )
                    
                    results.append({
                        'text': text,
                        'speaker': speaker_names[idx],
                        'confidence': calibrated_conf,
                        'start_time': seg['start_time'],
                        'end_time': seg['end_time']
                    })
            except:
                pass
            
            if os.path.exists(temp_path):
                os.remove(temp_path)
        
        if progress_callback:
            progress_callback("é›¶å®¹å¿å»é‡...")
        
        results = self.repetition_suppressor.detect_and_fix_segments(results)
        
        if progress_callback:
            progress_callback("æ™ºèƒ½æ ‡ç‚¹...")
        
        results = self.punctuation.restore(results)
        
        if progress_callback:
            progress_callback("åˆå¹¶ç»“æœ...")
        
        return self._merge_results(results)
    
    def _match_speakers(self, embeddings):
        names = []
        confidences = []
        
        for emb in embeddings:
            if emb is None or not self.registered_voices:
                names.append("æœªçŸ¥è¯´è¯äºº")
                confidences.append(0.0)
                continue
            
            emb_norm = normalize_embedding(emb)
            
            best_score = 0.0
            best_name = "æœªçŸ¥è¯´è¯äºº"
            
            for name, ref_emb in self.registered_voices.items():
                score = np.dot(emb_norm, ref_emb)
                if score > best_score:
                    best_score = score
                    best_name = name
            
            if best_score >= 0.65:
                names.append(best_name)
                confidences.append(best_score)
            else:
                names.append("æœªçŸ¥è¯´è¯äºº")
                confidences.append(0.0)
        
        return names, confidences
    
    def _merge_results(self, results):
        if not results:
            return []
        
        merged = []
        current = results[0].copy()
        
        for i in range(1, len(results)):
            next_item = results[i]
            
            if (current['speaker'] == next_item['speaker'] and 
                next_item['start_time'] - current['end_time'] < 2.0):
                
                if current['text'] and not current['text'][-1] in 'ã€‚!?':
                    current['text'] += ','
                
                current['text'] += next_item['text']
                current['end_time'] = next_item['end_time']
                current['confidence'] = (current['confidence'] + next_item['confidence']) / 2
            else:
                merged.append(current)
                current = next_item.copy()
        
        merged.append(current)
        return merged

# ==================== åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨ ====================

if 'model_manager' not in st.session_state:
    st.session_state.model_manager = ModelManager()

# ==================== ç³»ç»ŸçŠ¶æ€æ˜¾ç¤º ====================

col1, col2, col3, col4 = st.columns(4)

with col1:
    gpu_status = "GPU" if torch.cuda.is_available() else "CPU"
    st.metric("è¿è¡Œè®¾å¤‡", gpu_status)

with col2:
    model_status = "å·²åŠ è½½" if st.session_state.model_manager._asr else "æœªåŠ è½½"
    st.metric("æ¨¡å‹çŠ¶æ€", model_status)

with col3:
    voiceprint_count = len([f for f in os.listdir(VOICEPRINT_DIR) if f.endswith('.npy')])
    st.metric("å·²æ³¨å†Œå£°çº¹", voiceprint_count)

with col4:
    if torch.cuda.is_available():
        st.metric("GPUå†…å­˜", f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        st.metric("CPUæ ¸å¿ƒ", os.cpu_count())

# ==================== åŠ è½½æ¨¡å‹æŒ‰é’® ====================

if st.session_state.model_manager._asr is None:
    st.info("è¯·å…ˆåŠ è½½AIæ¨¡å‹")
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        if st.button("åŠ è½½AIå¼•æ“", type="primary", use_container_width=True):
            st.session_state.model_manager.load_models()
            st.balloons()
            time.sleep(1)
            st.rerun()
    
    st.stop()

# ==================== ä¾§è¾¹æ  - å£°çº¹ç®¡ç† ====================

st.sidebar.header("å£°çº¹ç®¡ç†")

voiceprint_files = [f for f in os.listdir(VOICEPRINT_DIR) if f.endswith('.npy')]
voiceprint_names = [os.path.splitext(f)[0] for f in voiceprint_files]

if voiceprint_names:
    st.sidebar.success(f"å·²æ³¨å†Œ: {len(voiceprint_names)} ä¸ª")
    with st.sidebar.expander("æŸ¥çœ‹å£°çº¹"):
        for name in voiceprint_names:
            st.sidebar.text(f"{name}")

with st.sidebar.form("register"):
    reg_name = st.text_input("å£°çº¹åç§°")
    reg_audio = st.file_uploader("ä¸Šä¼ éŸ³é¢‘", type=["wav", "mp3", "flac"])
    
    if st.form_submit_button("æ³¨å†Œ"):
        if reg_name and reg_audio:
            reg_path = os.path.join(TEMP_DIR, reg_audio.name)
            with open(reg_path, "wb") as f:
                f.write(reg_audio.getbuffer())
            
            try:
                res = st.session_state.model_manager._sv.generate(input=reg_path)
                if res and isinstance(res, list) and len(res) > 0:
                    item = res[0]
                    embedding = None
                    if isinstance(item, dict):
                        for key in ["embedding", "spk_embedding", "emb"]:
                            if key in item:
                                embedding = item[key]
                                break
                    
                    if embedding is not None:
                        emb_np = tensor_to_numpy(embedding)
                        save_path = os.path.join(VOICEPRINT_DIR, f"{reg_name}.npy")
                        np.save(save_path, emb_np)
                        st.sidebar.success(f"'{reg_name}' æ³¨å†ŒæˆåŠŸ!")
                        time.sleep(1)
                        st.rerun()
            except Exception as e:
                st.sidebar.error(f"æ³¨å†Œå¤±è´¥: {str(e)[:50]}")

# ==================== ä¾§è¾¹æ  - è®¾ç½® ====================

st.sidebar.header("ä¼˜åŒ–è®¾ç½®")

st.sidebar.subheader("åŸºç¡€è®¾ç½®")
enable_denoise = st.sidebar.checkbox("å¯ç”¨é™å™ª", True)
enable_diarization = st.sidebar.checkbox("å¯ç”¨è¯´è¯äººèšç±»", True)

st.sidebar.subheader("é«˜çº§æŠ€æœ¯")
enable_lm_fusion = st.sidebar.checkbox("å¯ç”¨è¯­è¨€æ¨¡å‹èåˆ", True)
enable_beam_search = st.sidebar.checkbox("å¯ç”¨Beam Search", True)
enable_adaptive_tuning = st.sidebar.checkbox("å¯ç”¨è‡ªé€‚åº”è°ƒä¼˜", True, help="æ ¹æ®éŸ³é¢‘ç‰¹å¾è‡ªåŠ¨è°ƒæ•´å‚æ•°")
lm_weight = st.sidebar.slider("LMèåˆæƒé‡", 0.1, 0.5, 0.3, 0.05)
beam_size = st.sidebar.slider("Beamå¤§å°", 3, 10, 5, 1)

st.sidebar.subheader("æ˜¾ç¤ºé€‰é¡¹")
show_timestamps = st.sidebar.checkbox("æ˜¾ç¤ºæ—¶é—´æˆ³", False)
show_confidence = st.sidebar.checkbox("æ˜¾ç¤ºç½®ä¿¡åº¦", True)
show_speaker_analysis = st.sidebar.checkbox("æ˜¾ç¤ºè¯´è¯äººåˆ†æ", True)

# ==================== ä¸»ç•Œé¢ - éŸ³é¢‘ç®¡ç† ====================

st.subheader("éŸ³é¢‘æ–‡ä»¶ç®¡ç†")

audio_files = [f for f in os.listdir(TEMP_DIR) if f.endswith(('.wav', '.mp3', '.flac', '.m4a'))]

col_left, col_right = st.columns([1, 1])

with col_left:
    st.markdown("**ä¸Šä¼ æ–°éŸ³é¢‘**")
    uploaded = st.file_uploader("æ”¯æŒ WAV, MP3, FLAC, M4A", type=["wav", "mp3", "flac", "m4a"])

with col_right:
    st.markdown("**å†å²éŸ³é¢‘æ–‡ä»¶**")
    if audio_files:
        selected_file = st.selectbox(
            f"é€‰æ‹©å·²æœ‰éŸ³é¢‘ ({len(audio_files)} ä¸ª)",
            [""] + audio_files,
            format_func=lambda x: "è¯·é€‰æ‹©..." if x == "" else x
        )
    else:
        st.info("æš‚æ— å†å²éŸ³é¢‘æ–‡ä»¶")
        selected_file = ""

audio_path = None
audio_name = None

if uploaded:
    audio_path = os.path.join(TEMP_DIR, uploaded.name)
    audio_name = uploaded.name
    with open(audio_path, "wb") as f:
        f.write(uploaded.getbuffer())
    st.success(f"å·²ä¸Šä¼ : {uploaded.name}")
elif selected_file:
    audio_path = os.path.join(TEMP_DIR, selected_file)
    audio_name = selected_file
    st.info(f"å·²é€‰æ‹©: {selected_file}")

if audio_path and os.path.exists(audio_path):
    st.audio(audio_path)
    
    speech, sr = sf.read(audio_path)
    if len(speech.shape) > 1:
        speech = speech.mean(axis=1)
    duration = len(speech) / sr
    
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("æ–‡ä»¶å", audio_name[:15] + "..." if len(audio_name) > 15 else audio_name)
    col2.metric("é‡‡æ ·ç‡", f"{sr} Hz")
    col3.metric("æ—¶é•¿", f"{duration:.1f} ç§’")
    col4.metric("å£°é“", "å•" if len(speech.shape) == 1 else "ç«‹ä½“")
    
    col_btn1, col_btn2, col_btn3 = st.columns([2, 1, 1])
    
    with col_btn2:
        if st.button("åˆ é™¤æ­¤æ–‡ä»¶", use_container_width=True):
            try:
                os.remove(audio_path)
                st.success("æ–‡ä»¶å·²åˆ é™¤")
                time.sleep(0.5)
                st.rerun()
            except Exception as e:
                st.error(f"åˆ é™¤å¤±è´¥: {e}")
    
    with col_btn3:
        if len(audio_files) > 0:
            if st.button("æ¸…ç©ºæ‰€æœ‰", use_container_width=True):
                try:
                    count = 0
                    for f in audio_files:
                        os.remove(os.path.join(TEMP_DIR, f))
                        count += 1
                    st.success(f"å·²æ¸…ç©º {count} ä¸ªæ–‡ä»¶")
                    time.sleep(0.5)
                    st.rerun()
                except Exception as e:
                    st.error(f"æ¸…ç©ºå¤±è´¥: {e}")
    
    st.markdown("---")
    
    if st.button("å¼€å§‹è¯†åˆ«", type="primary", use_container_width=True):
        # ä½¿ç”¨å•†ä¸šçº§è¯†åˆ«å¼•æ“
        engine = CommercialGradeRecognitionEngine(
            st.session_state.model_manager._asr,
            st.session_state.model_manager._sv,
            st.session_state.model_manager._vad,
            VOICEPRINT_DIR,
            enable_denoise=enable_denoise,
            enable_diarization=enable_diarization,
            enable_lm_fusion=enable_lm_fusion,
            enable_beam_search=enable_beam_search,
            enable_batch_processing=True,
            enable_adaptive_tuning=enable_adaptive_tuning
        )
        
        status = st.empty()
        start_time = time.time()
        
        def update_status(msg):
            status.info(msg)
        
        try:
            results = engine.process_audio(audio_path, update_status)
            end_time = time.time()
            
            status.empty()
            
            if results:
                # ==================== ã€å…³é”®ä¿®å¤ã€‘æ·±åº¦æ–‡æœ¬å»é‡é˜²å¾¡å±‚ ====================
                st.markdown("### ğŸ”„ æ­£åœ¨æ‰§è¡Œæ·±åº¦é‡å¤æŠ‘åˆ¶...")

                # æ–°å¢ï¼šä¸“æ€æ•´å¥å®Œå…¨é‡å¤ï¼ˆé’ˆå¯¹"èˆ†è®ºèˆ†è®ºæ„è¯†"20è¿å‘ï¼‰
                def remove_exact_long_sentence_repetitions(text, min_len=15):
                    import re
                    sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]', text)
                    sentences = [s.strip() for s in sentences if s.strip()]

                    if len(sentences) <= 1:
                        return text

                    cleaned = []
                    seen = set()

                    for sent in sentences:
                        if len(sent) < min_len:
                            cleaned.append(sent)
                            continue
                        if sent in seen:
                            continue  # ç›´æ¥ä¸¢å¼ƒå®Œå…¨ç›¸åŒçš„é•¿å¥
                        seen.add(sent)
                        cleaned.append(sent)

                    result = 'ã€‚'.join(cleaned)
                    if text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›')):
                        result += 'ã€‚'
                    return result

                # å¯¹æ¯ä¸ªæ®µè½è¿›è¡Œå¤šå±‚å»é‡
                for item in results:
                    text = item['text']

                    # ç¬¬ä¸€å±‚ï¼šæ€æ•´å¥å®Œå…¨é‡å¤ï¼ˆæœ€æœ‰æ•ˆï¼‰
                    text = remove_exact_long_sentence_repetitions(text)

                    # ç¬¬äºŒå±‚ï¼šè°ƒç”¨ä½ åŸæœ‰çš„å·¥ä¸šçº§å»é‡
                    text = IndustrialRepetitionSuppressor.aggressive_dedup(text)

                    # ç¬¬ä¸‰å±‚ï¼šè¯­ä¹‰ç›¸ä¼¼å»é‡
                    text = IndustrialRepetitionSuppressor._remove_semantic_repetitions(text)

                    item['text'] = text.strip()

                # ç¬¬å››å±‚ï¼šè·¨æ®µè½å»é‡ï¼ˆé˜²æ­¢ä¸åŒæ—¶é—´æˆ³å‡ºç°åŒä¸€å¥è¯ï¼‰
                seen_cross_segment = set()
                unique_results = []
                for r in results:
                    text = r['text']
                    if len(text) > 20 and text in seen_cross_segment:
                        continue  # è·¨æ®µè½å®Œå…¨é‡å¤ï¼Œä¸¢å¼ƒ
                    if len(text) > 20:
                        seen_cross_segment.add(text)
                    unique_results.append(r)

                results = unique_results

                # é¢å¤–æƒ©ç½šæ˜æ˜¾é‡å¤ç”Ÿæˆçš„æ®µè½
                for r in results:
                    text = r['text']
                    if len(text) > 50:
                        words = list(text)
                        repeat_rate = sum(1 for i in range(1, len(words)) if words[i] == words[i-1]) / len(words)
                        if repeat_rate > 0.1:  # é«˜åº¦å­—ç¬¦é‡å¤
                            r['confidence'] *= 0.2

                st.success("âœ… æ·±åº¦å»é‡å®Œæˆï¼Œå·²æ¸…é™¤ä¸¥é‡é‡å¤å†…å®¹")

                st.success(f"è¯†åˆ«å®Œæˆ! ç”¨æ—¶ {end_time - start_time:.1f} ç§’")

                total_chars = sum(len(r['text']) for r in results)
                unique_speakers = len(set(r['speaker'] for r in results))
                avg_confidence = np.mean([r['confidence'] for r in results])
                high_conf_ratio = sum(1 for r in results if r['confidence'] > 0.85) / len(results)
                
                col1, col2, col3, col4, col5 = st.columns(5)
                col1.metric("æ€»å­—ç¬¦", total_chars)
                col2.metric("è¯´è¯äººæ•°", unique_speakers)
                col3.metric("å¯¹è¯æ®µæ•°", len(results))
                col4.metric("å¹³å‡ç½®ä¿¡åº¦", f"{avg_confidence:.2%}")
                col5.metric("é«˜ç½®ä¿¡åº¦æ¯”ä¾‹", f"{high_conf_ratio:.1%}")
                
                if show_speaker_analysis and unique_speakers > 1:
                    st.subheader("è¯´è¯äººåˆ†æ")
                    
                    speaker_stats = {}
                    for r in results:
                        spk = r['speaker']
                        if spk not in speaker_stats:
                            speaker_stats[spk] = {
                                'segments': 0, 'chars': 0, 'duration': 0.0, 'avg_conf': []
                            }
                        speaker_stats[spk]['segments'] += 1
                        speaker_stats[spk]['chars'] += len(r['text'])
                        speaker_stats[spk]['duration'] += r['end_time'] - r['start_time']
                        speaker_stats[spk]['avg_conf'].append(r['confidence'])
                    
                    cols = st.columns(unique_speakers)
                    for idx, (spk, stats) in enumerate(speaker_stats.items()):
                        with cols[idx]:
                            st.markdown(f"**{spk}**")
                            st.metric("å‘è¨€æ®µæ•°", stats['segments'])
                            st.metric("å­—ç¬¦æ•°", stats['chars'])
                            st.metric("æ—¶é•¿", f"{stats['duration']:.1f}s")
                            st.metric("ç½®ä¿¡åº¦", f"{np.mean(stats['avg_conf']):.2%}")
                
                st.subheader("è¯†åˆ«ç»“æœ")
                
                for idx, item in enumerate(results):
                    display = f"**{item['speaker']}**"
                    if show_confidence:
                        conf = item['confidence']
                        if conf > 0.90:
                            conf_color = "ğŸŸ¢"
                        elif conf > 0.80:
                            conf_color = "ğŸŸ¡"
                        elif conf > 0.70:
                            conf_color = "ğŸŸ "
                        else:
                            conf_color = "ğŸ”´"
                        display += f" {conf_color} `{conf:.2%}`"
                    if show_timestamps:
                        display += f" *[{item['start_time']:.1f}s-{item['end_time']:.1f}s]*"
                    display += f": {item['text']}"
                    st.markdown(display)
                    if idx < len(results) - 1:
                        st.markdown("---")
                
                st.subheader("å¯¼å‡ºç»“æœ")
                
                export_text = "\n\n".join([f"{r['speaker']}: {r['text']}" for r in results])
                
                col_export1, col_export2, col_export3, col_export4 = st.columns(4)
                
                with col_export1:
                    st.download_button(
                        "ä¸‹è½½TXT",
                        export_text,
                        f"transcript_{audio_name}.txt",
                        "text/plain",
                        use_container_width=True
                    )
                
                with col_export2:
                    detailed_text = "\n".join([
                        f"[{r['start_time']:.1f}s-{r['end_time']:.1f}s] {r['speaker']} ({r['confidence']:.2%}): {r['text']}"
                        for r in results
                    ])
                    st.download_button(
                        "ä¸‹è½½è¯¦ç»†ç‰ˆ",
                        detailed_text,
                        f"transcript_detailed_{audio_name}.txt",
                        "text/plain",
                        use_container_width=True
                    )
                
                with col_export3:
                    srt_text = ""
                    for i, r in enumerate(results, 1):
                        start_h = int(r['start_time'] // 3600)
                        start_m = int((r['start_time'] % 3600) // 60)
                        start_s = r['start_time'] % 60
                        start_str = f"{start_h:02d}:{start_m:02d}:{start_s:06.3f}".replace('.', ',')
                        
                        end_h = int(r['end_time'] // 3600)
                        end_m = int((r['end_time'] % 3600) // 60)
                        end_s = r['end_time'] % 60
                        end_str = f"{end_h:02d}:{end_m:02d}:{end_s:06.3f}".replace('.', ',')
                        
                        srt_text += f"{i}\n{start_str} --> {end_str}\n{r['speaker']}: {r['text']}\n\n"
                    
                    st.download_button(
                        "ä¸‹è½½SRTå­—å¹•",
                        srt_text,
                        f"subtitle_{audio_name}.srt",
                        "text/plain",
                        use_container_width=True
                    )
                
                with col_export4:
                    json_data = {
                        'metadata': {
                            'filename': audio_name,
                            'duration': duration,
                            'num_speakers': unique_speakers,
                            'num_segments': len(results),
                            'avg_confidence': float(avg_confidence),
                            'processing_time': end_time - start_time
                        },
                        'results': [
                            {
                                'index': i,
                                'speaker': r['speaker'],
                                'text': r['text'],
                                'confidence': float(r['confidence']),
                                'start_time': float(r['start_time']),
                                'end_time': float(r['end_time'])
                            }
                            for i, r in enumerate(results, 1)
                        ]
                    }
                    
                    st.download_button(
                        "ä¸‹è½½JSON",
                        json.dumps(json_data, ensure_ascii=False, indent=2),
                        f"transcript_{audio_name}.json",
                        "application/json",
                        use_container_width=True
                    )
            else:
                st.warning("æœªè¯†åˆ«åˆ°å†…å®¹")
        
        except Exception as e:
            st.error(f"è¯†åˆ«å‡ºé”™: {e}")
            import traceback
            with st.expander("æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯"):
                st.code(traceback.format_exc())

else:
    st.info("è¯·ä¸Šä¼ æ–°éŸ³é¢‘æˆ–é€‰æ‹©å†å²æ–‡ä»¶å¼€å§‹è¯†åˆ«")
    
    if audio_files:
        total_size = sum(os.path.getsize(os.path.join(TEMP_DIR, f)) for f in audio_files) / (1024 * 1024)
        st.caption(f"å½“å‰å­˜å‚¨: {len(audio_files)} ä¸ªæ–‡ä»¶, å…± {total_size:.1f} MB")

# ==================== é¡µè„š ====================

st.markdown("---")

footer_cols = st.columns([2, 1, 1])
with footer_cols[0]:
    st.caption("AudioTrans v1.0")
with footer_cols[1]:
    tech_enabled = []
    if enable_lm_fusion:
        tech_enabled.append("LM")
    if enable_beam_search:
        tech_enabled.append("Beam")
    if enable_denoise:
        tech_enabled.append("é™å™ª")
    if enable_diarization:
        tech_enabled.append("èšç±»")
    
    st.caption(f"å·²å¯ç”¨: {', '.join(tech_enabled) if tech_enabled else 'åŸºç¡€æ¨¡å¼'}")
with footer_cols[2]:
    st.caption(f"å­˜å‚¨: {len(audio_files)} æ–‡ä»¶")
