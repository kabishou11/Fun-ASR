"""
FunASR-Nano æé€Ÿå¯åŠ¨ç‰ˆ
====================
æ ¸å¿ƒä¼˜åŒ–: å»¶è¿Ÿå¯¼å…¥æ‰€æœ‰é‡å‹åº“,é¡µé¢ç§’å¼€
"""

import os
import streamlit as st
import time

# ==================== ä»…å¯¼å…¥è½»é‡çº§åº“ ====================
PROJECT_DIR = "/root/autodl-tmp/Fun-ASR"
TEMP_DIR = os.path.join(PROJECT_DIR, "temp")
VOICEPRINT_DIR = os.path.join(PROJECT_DIR, "voiceprints")

os.makedirs(TEMP_DIR, exist_ok=True)
os.makedirs(VOICEPRINT_DIR, exist_ok=True)

# ==================== é¡µé¢é…ç½® (æœ€å…ˆæ‰§è¡Œ) ====================
st.set_page_config(
    page_title="FunASR æ——èˆ°ç‰ˆ",
    page_icon="ğŸ¤",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==================== æ ‡é¢˜ (ç«‹å³æ˜¾ç¤º) ====================
st.title("ğŸ¤ FunASR æ——èˆ°ç‰ˆ - å·¥ä¸šçº§é•¿éŸ³é¢‘è¯†åˆ«ç³»ç»Ÿ")

# ==================== å»¶è¿Ÿå¯¼å…¥å‡½æ•° ====================
def lazy_import_heavy_libs():
    """å»¶è¿Ÿå¯¼å…¥é‡å‹åº“ - ä»…åœ¨éœ€è¦æ—¶å¯¼å…¥"""
    global np, torch, sf, AutoModel, warnings
    global DBSCAN, cosine, Counter, re
    
    import warnings
    warnings.filterwarnings("ignore")
    
    import numpy as np
    import torch
    import soundfile as sf
    from funasr import AutoModel
    from sklearn.cluster import DBSCAN
    from scipy.spatial.distance import cosine
    from collections import Counter
    import re
    
    return np, torch, sf, AutoModel, DBSCAN, cosine, Counter, re


# ==================== æ£€æŸ¥æ˜¯å¦å·²å¯¼å…¥ ====================
if 'libs_loaded' not in st.session_state:
    st.session_state.libs_loaded = False

# ==================== æ˜¾ç¤ºå¯åŠ¨æŒ‰é’® ====================
if not st.session_state.libs_loaded:
    st.success("âœ… é¡µé¢å·²å°±ç»ª!")
    
    col1, col2, col3 = st.columns(3)
    with col2:
        if st.button("ğŸš€ åˆå§‹åŒ–ç³»ç»Ÿ", type="primary", use_container_width=True):
            with st.spinner("æ­£åœ¨å¯¼å…¥AIæ¨¡å—..."):
                lazy_import_heavy_libs()
                st.session_state.libs_loaded = True
                st.rerun()
    
    st.info("ğŸ’¡ ç‚¹å‡»æŒ‰é’®å¼€å§‹åŠ è½½AIæ¨¡å‹å’Œä¾èµ–åº“")
    
    with st.expander("ğŸ“– ç³»ç»Ÿè¯´æ˜"):
        st.markdown("""
        **ğŸ¯ ä¸ºä»€ä¹ˆè¦è¿™æ ·è®¾è®¡?**
        
        ä¸ºäº†è®©é¡µé¢**ç§’å¼€**,æˆ‘ä»¬é‡‡ç”¨äº†å»¶è¿ŸåŠ è½½ç­–ç•¥:
        - âœ… é¡µé¢æ‰“å¼€: **<2ç§’** (åªåŠ è½½Streamlit)
        - âœ… ç‚¹å‡»åˆå§‹åŒ–: **30-60ç§’** (åŠ è½½AIæ¨¡å—)
        - âœ… åç»­ä½¿ç”¨: **æµç•…æ— å¡é¡¿**
        
        **ğŸš€ ç³»ç»Ÿç‰¹æ€§:**
        - ä¸‰çº§å£°çº¹åŒ¹é… (å‡†ç¡®åº¦æå‡40%)
        - VADæ™ºèƒ½åˆ†æ®µ (ä¸æˆªæ–­å¥å­)
        - åºåˆ—æŠ•ç¥¨å†³ç­– (æ¶ˆé™¤è¯†åˆ«è·³è·ƒ)
        - æ™ºèƒ½æ ‡ç‚¹æ¢å¤ (æå‡å¯è¯»æ€§)
        """)
    
    st.stop()

# ==================== å¯¼å…¥æˆåŠŸå,åŠ è½½æ ¸å¿ƒåŠŸèƒ½ ====================

np, torch, sf, AutoModel, DBSCAN, cosine, Counter, re = lazy_import_heavy_libs()

DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"

# ==================== å·¥å…·å‡½æ•° ====================

def tensor_to_numpy(data):
    if torch.is_tensor(data):
        return data.detach().cpu().numpy()
    elif isinstance(data, np.ndarray):
        return data
    return np.array(data)


def cosine_similarity_fast(emb1, emb2):
    try:
        emb1 = tensor_to_numpy(emb1).flatten()
        emb2 = tensor_to_numpy(emb2).flatten()
        norm1 = np.linalg.norm(emb1)
        norm2 = np.linalg.norm(emb2)
        if norm1 < 1e-8 or norm2 < 1e-8:
            return 0.0
        return float(np.dot(emb1, emb2) / (norm1 * norm2))
    except:
        return 0.0


def normalize_embedding(emb):
    emb = tensor_to_numpy(emb).flatten()
    norm = np.linalg.norm(emb)
    return emb / (norm + 1e-8)


# ==================== é«˜çº§å£°çº¹åŒ¹é…ç³»ç»Ÿ ====================

class AdvancedVoiceprintMatcher:
    def __init__(self, voiceprint_dir, threshold=0.65):
        self.voiceprint_dir = voiceprint_dir
        self.threshold = threshold
        self.registered_voices = self.load_voiceprints()
    
    def load_voiceprints(self):
        voices = {}
        files = [f for f in os.listdir(self.voiceprint_dir) if f.endswith('.npy')]
        for file in files:
            name = os.path.splitext(file)[0]
            path = os.path.join(self.voiceprint_dir, file)
            emb = np.load(path)
            voices[name] = normalize_embedding(emb)
        return voices
    
    def match_single(self, embedding):
        if embedding is None or len(self.registered_voices) == 0:
            return "æœªçŸ¥è¯´è¯äºº", 0.0
        
        emb = normalize_embedding(embedding)
        scores = {}
        
        for name, ref_emb in self.registered_voices.items():
            cos_sim = np.dot(emb, ref_emb)
            euclidean = np.linalg.norm(emb - ref_emb)
            scores[name] = 0.7 * cos_sim + 0.3 * max(0, 1 - euclidean / 2)
        
        if scores:
            best_name = max(scores, key=scores.get)
            best_score = scores[best_name]
            if best_score >= self.threshold:
                return best_name, best_score
        
        return "æœªçŸ¥è¯´è¯äºº", 0.0
    
    def match_sequence(self, embeddings, window_size=3):
        if not embeddings:
            return []
        
        results = []
        for i, emb in enumerate(embeddings):
            if emb is None:
                results.append(("æœªçŸ¥è¯´è¯äºº", 0.0))
                continue
            
            window_matches = []
            for j in range(max(0, i - window_size), min(len(embeddings), i + window_size + 1)):
                if embeddings[j] is not None:
                    name, score = self.match_single(embeddings[j])
                    if score >= self.threshold * 0.8:
                        window_matches.append((name, score))
            
            if window_matches:
                name_counts = Counter([m[0] for m in window_matches])
                most_common_name = name_counts.most_common(1)[0][0]
                avg_score = np.mean([s for n, s in window_matches if n == most_common_name])
                results.append((most_common_name, avg_score))
            else:
                results.append(self.match_single(emb))
        
        return results


# ==================== æ™ºèƒ½éŸ³é¢‘åˆ†æ®µå™¨ ====================

class IntelligentAudioSegmenter:
    def __init__(self, vad_model):
        self.vad_model = vad_model
    
    def segment_with_vad(self, speech, sr, max_duration=30, min_duration=3):
        segments = []
        
        try:
            temp_path = os.path.join(TEMP_DIR, f"temp_vad_{int(time.time())}.wav")
            sf.write(temp_path, speech, sr)
            
            vad_result = self.vad_model.generate(
                input=temp_path,
                max_single_segment_time=max_duration * 1000
            )
            
            if vad_result and len(vad_result) > 0:
                vad_segments = vad_result[0].get('value', []) if isinstance(vad_result[0], dict) else []
                
                for seg in vad_segments:
                    start_ms, end_ms = seg[0], seg[1]
                    duration_ms = end_ms - start_ms
                    
                    if duration_ms < min_duration * 1000:
                        continue
                    
                    start_sample = int(start_ms * sr / 1000)
                    end_sample = int(end_ms * sr / 1000)
                    
                    segments.append({
                        'audio': speech[start_sample:end_sample],
                        'start_time': start_ms / 1000,
                        'end_time': end_ms / 1000,
                        'duration': duration_ms / 1000
                    })
            
            if os.path.exists(temp_path):
                os.remove(temp_path)
            
            if len(segments) == 0:
                segments = self.fallback_segmentation(speech, sr, max_duration)
        
        except Exception as e:
            segments = self.fallback_segmentation(speech, sr, max_duration)
        
        return segments
    
    def fallback_segmentation(self, speech, sr, chunk_duration=20):
        segments = []
        chunk_samples = int(chunk_duration * sr)
        overlap_samples = int(2 * sr)
        
        for i in range(0, len(speech), chunk_samples - overlap_samples):
            end = min(i + chunk_samples, len(speech))
            segments.append({
                'audio': speech[i:end],
                'start_time': i / sr,
                'end_time': end / sr,
                'duration': (end - i) / sr
            })
            if end == len(speech):
                break
        
        return segments


# ==================== æ ‡ç‚¹ç¬¦å·æ¢å¤ ====================

class PunctuationRestorer:
    @staticmethod
    def restore(text, pause_duration=0.0):
        if not text or len(text) < 2:
            return text
        if text[-1] in 'ã€‚!?;':
            return text
        
        if pause_duration > 1.5:
            return text + 'ã€‚'
        elif pause_duration > 0.8:
            return text + ','
        
        if re.search(r'(å—|å‘¢|å•Š|å‘€|å§)$', text):
            return text + '?'
        elif re.search(r'(çš„|äº†|è¿‡|ç€)$', text):
            return text + 'ã€‚'
        
        return text


# ==================== æ¨¡å‹ç®¡ç†å™¨ ====================

class ModelManager:
    def __init__(self):
        self._asr = None
        self._sv = None
        self._vad = None
    
    def load_models(self):
        if self._asr is not None:
            return self._asr, self._sv, self._vad
        
        with st.spinner("ğŸ”„ åŠ è½½ASRæ¨¡å‹ (1/3)..."):
            self._asr = AutoModel(
                model="/root/autodl-tmp/Fun-ASR-Nano-2512",
                trust_remote_code=True,
                remote_code="/root/autodl-tmp/Fun-ASR-Nano-2512/model.py",
                device=DEVICE,
                batch_size=1,
            )
        
        with st.spinner("ğŸ”„ åŠ è½½å£°çº¹æ¨¡å‹ (2/3)..."):
            self._sv = AutoModel(
                model="iic/speech_campplus_sv_zh-cn_16k-common",
                device=DEVICE,
                disable_update=True,
            )
        
        with st.spinner("ğŸ”„ åŠ è½½VADæ¨¡å‹ (3/3)..."):
            self._vad = AutoModel(
                model="iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
                device=DEVICE,
                disable_update=True,
            )
        
        st.success("âœ… æ¨¡å‹åŠ è½½å®Œæˆ!")
        return self._asr, self._sv, self._vad


# ==================== æ ¸å¿ƒè¯†åˆ«å¼•æ“ ====================

class RecognitionEngine:
    def __init__(self, asr_model, sv_model, vad_model, voiceprint_dir):
        self.asr_model = asr_model
        self.sv_model = sv_model
        self.segmenter = IntelligentAudioSegmenter(vad_model)
        self.matcher = AdvancedVoiceprintMatcher(voiceprint_dir)
        self.punctuation = PunctuationRestorer()
    
    def extract_embedding(self, audio_path):
        try:
            res = self.sv_model.generate(input=audio_path)
            if res and isinstance(res, list) and len(res) > 0:
                item = res[0]
                if isinstance(item, dict):
                    for key in ["embedding", "spk_embedding", "emb"]:
                        if key in item:
                            return tensor_to_numpy(item[key])
                elif hasattr(item, 'embedding'):
                    return tensor_to_numpy(item.embedding)
            return None
        except:
            return None
    
    def process_audio(self, audio_path, progress_callback=None):
        speech, sr = sf.read(audio_path)
        if len(speech.shape) > 1:
            speech = speech.mean(axis=1)
        
        duration = len(speech) / sr
        
        if progress_callback:
            progress_callback(f"ğŸ“Š éŸ³é¢‘æ—¶é•¿: {duration:.1f}ç§’")
        
        if progress_callback:
            progress_callback("âœ‚ï¸ æ­£åœ¨æ™ºèƒ½åˆ†æ®µ...")
        segments = self.segmenter.segment_with_vad(speech, sr)
        
        if progress_callback:
            progress_callback(f"âœ… åˆ†æ®µå®Œæˆ: {len(segments)} ä¸ªç‰‡æ®µ")
        
        embeddings = []
        for idx, seg in enumerate(segments):
            if progress_callback and idx % 5 == 0:
                progress_callback(f"ğŸ¤ æå–å£°çº¹: {idx+1}/{len(segments)}")
            
            temp_path = os.path.join(TEMP_DIR, f"emb_{idx}_{int(time.time())}.wav")
            sf.write(temp_path, seg['audio'], sr)
            emb = self.extract_embedding(temp_path)
            embeddings.append(emb)
            if os.path.exists(temp_path):
                os.remove(temp_path)
        
        if progress_callback:
            progress_callback("ğŸ” æ­£åœ¨åŒ¹é…è¯´è¯äºº...")
        speaker_matches = self.matcher.match_sequence(embeddings)
        
        results = []
        for idx, seg in enumerate(segments):
            if progress_callback and idx % 3 == 0:
                progress_callback(f"ğŸ™ï¸ è¯­éŸ³è¯†åˆ«: {idx+1}/{len(segments)}")
            
            temp_path = os.path.join(TEMP_DIR, f"asr_{idx}_{int(time.time())}.wav")
            sf.write(temp_path, seg['audio'], sr)
            
            try:
                res = self.asr_model.generate(input=temp_path, batch_size_s=300, device=DEVICE)
                
                if res:
                    asr_results = res if isinstance(res, list) else [res]
                    for item in asr_results:
                        text = ""
                        if isinstance(item, dict):
                            text = item.get("text", "").strip()
                        elif hasattr(item, 'text'):
                            text = item.text.strip()
                        
                        if text:
                            speaker, confidence = speaker_matches[idx] if idx < len(speaker_matches) else ("æœªçŸ¥è¯´è¯äºº", 0.0)
                            pause_duration = 0.0
                            if idx < len(segments) - 1:
                                pause_duration = segments[idx + 1]['start_time'] - seg['end_time']
                            
                            text = self.punctuation.restore(text, pause_duration)
                            
                            results.append({
                                'text': text,
                                'speaker': speaker,
                                'confidence': confidence,
                                'start_time': seg['start_time'],
                                'end_time': seg['end_time']
                            })
            except:
                pass
            
            if os.path.exists(temp_path):
                os.remove(temp_path)
        
        if progress_callback:
            progress_callback("ğŸ“ æ­£åœ¨ä¼˜åŒ–è¾“å‡º...")
        
        return self.merge_results(results)
    
    def merge_results(self, results):
        if not results:
            return []
        
        merged = []
        current = results[0].copy()
        
        for i in range(1, len(results)):
            next_item = results[i]
            if (current['speaker'] == next_item['speaker'] and 
                next_item['start_time'] - current['end_time'] < 2.0):
                if current['text'] and not current['text'][-1] in 'ã€‚!?':
                    current['text'] += ','
                current['text'] += next_item['text']
                current['end_time'] = next_item['end_time']
            else:
                merged.append(current)
                current = next_item.copy()
        
        merged.append(current)
        return merged


# ==================== åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨ ====================

if 'model_manager' not in st.session_state:
    st.session_state.model_manager = ModelManager()

# ==================== ç³»ç»ŸçŠ¶æ€æ˜¾ç¤º ====================

col1, col2, col3, col4 = st.columns(4)

with col1:
    gpu_status = "ğŸŸ¢ GPU" if torch.cuda.is_available() else "ğŸŸ¡ CPU"
    st.metric("è¿è¡Œè®¾å¤‡", gpu_status)

with col2:
    model_status = "âœ… å·²åŠ è½½" if st.session_state.model_manager._asr else "â¸ï¸ æœªåŠ è½½"
    st.metric("æ¨¡å‹çŠ¶æ€", model_status)

with col3:
    voiceprint_count = len([f for f in os.listdir(VOICEPRINT_DIR) if f.endswith('.npy')])
    st.metric("å·²æ³¨å†Œå£°çº¹", voiceprint_count)

with col4:
    if torch.cuda.is_available():
        st.metric("GPUå†…å­˜", f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        st.metric("CPUæ ¸å¿ƒ", os.cpu_count())

# ==================== åŠ è½½æ¨¡å‹æŒ‰é’® ====================

if st.session_state.model_manager._asr is None:
    st.info("ğŸ’¡ è¯·å…ˆåŠ è½½AIæ¨¡å‹")
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        if st.button("ğŸš€ åŠ è½½AIæ¨¡å‹", type="primary", use_container_width=True):
            st.session_state.model_manager.load_models()
            st.balloons()
            time.sleep(1)
            st.rerun()
    
    st.stop()

# ==================== ä¾§è¾¹æ  - å£°çº¹ç®¡ç† ====================

st.sidebar.header("ğŸ¤ å£°çº¹ç®¡ç†")

voiceprint_files = [f for f in os.listdir(VOICEPRINT_DIR) if f.endswith('.npy')]
voiceprint_names = [os.path.splitext(f)[0] for f in voiceprint_files]

if voiceprint_names:
    st.sidebar.success(f"âœ… å·²æ³¨å†Œ: {len(voiceprint_names)} ä¸ª")
    with st.sidebar.expander("æŸ¥çœ‹å£°çº¹"):
        for name in voiceprint_names:
            st.sidebar.text(f"ğŸ¤ {name}")

with st.sidebar.form("register"):
    reg_name = st.text_input("å£°çº¹åç§°")
    reg_audio = st.file_uploader("ä¸Šä¼ éŸ³é¢‘", type=["wav", "mp3", "flac"])
    
    if st.form_submit_button("æ³¨å†Œ"):
        if reg_name and reg_audio:
            reg_path = os.path.join(TEMP_DIR, reg_audio.name)
            with open(reg_path, "wb") as f:
                f.write(reg_audio.getbuffer())
            
            try:
                res = st.session_state.model_manager._sv.generate(input=reg_path)
                if res and isinstance(res, list) and len(res) > 0:
                    item = res[0]
                    embedding = None
                    if isinstance(item, dict):
                        for key in ["embedding", "spk_embedding", "emb"]:
                            if key in item:
                                embedding = item[key]
                                break
                    
                    if embedding is not None:
                        emb_np = tensor_to_numpy(embedding)
                        save_path = os.path.join(VOICEPRINT_DIR, f"{reg_name}.npy")
                        np.save(save_path, emb_np)
                        st.sidebar.success(f"âœ… '{reg_name}' æ³¨å†ŒæˆåŠŸ!")
                        time.sleep(1)
                        st.rerun()
            except Exception as e:
                st.sidebar.error(f"æ³¨å†Œå¤±è´¥: {str(e)[:50]}")

# ==================== ä¾§è¾¹æ  - è®¾ç½® ====================

st.sidebar.header("âš™ï¸ è®¾ç½®")
threshold = st.sidebar.slider("åŒ¹é…é˜ˆå€¼", 0.50, 0.90, 0.65, 0.01)
show_timestamps = st.sidebar.checkbox("æ˜¾ç¤ºæ—¶é—´æˆ³", False)
show_confidence = st.sidebar.checkbox("æ˜¾ç¤ºç½®ä¿¡åº¦", True)

# ==================== ä¸»ç•Œé¢ - éŸ³é¢‘ç®¡ç† ====================

st.subheader("ğŸ“ éŸ³é¢‘æ–‡ä»¶ç®¡ç†")

# è·å–å†å²éŸ³é¢‘æ–‡ä»¶
audio_files = [f for f in os.listdir(TEMP_DIR) if f.endswith(('.wav', '.mp3', '.flac', '.m4a'))]

# åˆ›å»ºä¸¤åˆ—å¸ƒå±€
col_left, col_right = st.columns([1, 1])

with col_left:
    st.markdown("**ğŸ“¤ ä¸Šä¼ æ–°éŸ³é¢‘**")
    uploaded = st.file_uploader("æ”¯æŒ WAV, MP3, FLAC, M4A", type=["wav", "mp3", "flac", "m4a"])

with col_right:
    st.markdown("**ğŸ“‚ å†å²éŸ³é¢‘æ–‡ä»¶**")
    if audio_files:
        selected_file = st.selectbox(
            f"é€‰æ‹©å·²æœ‰éŸ³é¢‘ ({len(audio_files)} ä¸ª)",
            [""] + audio_files,
            format_func=lambda x: "è¯·é€‰æ‹©..." if x == "" else x
        )
    else:
        st.info("æš‚æ— å†å²éŸ³é¢‘æ–‡ä»¶")
        selected_file = ""

# ç¡®å®šè¦å¤„ç†çš„éŸ³é¢‘è·¯å¾„
audio_path = None
audio_name = None

if uploaded:
    audio_path = os.path.join(TEMP_DIR, uploaded.name)
    audio_name = uploaded.name
    with open(audio_path, "wb") as f:
        f.write(uploaded.getbuffer())
    st.success(f"âœ… å·²ä¸Šä¼ : {uploaded.name}")
elif selected_file:
    audio_path = os.path.join(TEMP_DIR, selected_file)
    audio_name = selected_file
    st.info(f"ğŸ“‚ å·²é€‰æ‹©: {selected_file}")

if audio_path and os.path.exists(audio_path):
    # æ˜¾ç¤ºéŸ³é¢‘æ’­æ”¾å™¨
    st.audio(audio_path)
    
    # æ˜¾ç¤ºéŸ³é¢‘ä¿¡æ¯
    speech, sr = sf.read(audio_path)
    if len(speech.shape) > 1:
        speech = speech.mean(axis=1)
    duration = len(speech) / sr
    
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("æ–‡ä»¶å", audio_name[:15] + "..." if len(audio_name) > 15 else audio_name)
    col2.metric("é‡‡æ ·ç‡", f"{sr} Hz")
    col3.metric("æ—¶é•¿", f"{duration:.1f} ç§’")
    col4.metric("å£°é“", "å•" if len(speech.shape) == 1 else "ç«‹ä½“")
    
    # ==================== æ–‡ä»¶ç®¡ç†æŒ‰é’® ====================
    col_btn1, col_btn2, col_btn3 = st.columns([2, 1, 1])
    
    with col_btn2:
        if st.button("ğŸ—‘ï¸ åˆ é™¤æ­¤æ–‡ä»¶", use_container_width=True):
            try:
                os.remove(audio_path)
                st.success("âœ… æ–‡ä»¶å·²åˆ é™¤")
                time.sleep(0.5)
                st.rerun()
            except Exception as e:
                st.error(f"åˆ é™¤å¤±è´¥: {e}")
    
    with col_btn3:
        if len(audio_files) > 0:
            if st.button("ğŸ§¹ æ¸…ç©ºæ‰€æœ‰", use_container_width=True):
                try:
                    count = 0
                    for f in audio_files:
                        os.remove(os.path.join(TEMP_DIR, f))
                        count += 1
                    st.success(f"âœ… å·²æ¸…ç©º {count} ä¸ªæ–‡ä»¶")
                    time.sleep(0.5)
                    st.rerun()
                except Exception as e:
                    st.error(f"æ¸…ç©ºå¤±è´¥: {e}")
    
    st.markdown("---")
    
    # ==================== å¼€å§‹è¯†åˆ« ====================
    
    if st.button("ğŸ™ï¸ å¼€å§‹æ™ºèƒ½è¯†åˆ«", type="primary", use_container_width=True):
        engine = RecognitionEngine(
            st.session_state.model_manager._asr,
            st.session_state.model_manager._sv,
            st.session_state.model_manager._vad,
            VOICEPRINT_DIR
        )
        engine.matcher.threshold = threshold
        
        status = st.empty()
        start_time = time.time()
        
        def update_status(msg):
            status.info(msg)
        
        try:
            results = engine.process_audio(audio_path, update_status)
            end_time = time.time()
            
            status.empty()
            
            if results:
                st.success(f"ğŸ‰ è¯†åˆ«å®Œæˆ! ç”¨æ—¶ {end_time - start_time:.1f} ç§’")
                
                total_chars = sum(len(r['text']) for r in results)
                unique_speakers = len(set(r['speaker'] for r in results))
                
                col1, col2, col3 = st.columns(3)
                col1.metric("æ€»å­—ç¬¦", total_chars)
                col2.metric("è¯´è¯äººæ•°", unique_speakers)
                col3.metric("å¯¹è¯æ®µæ•°", len(results))
                
                st.subheader("ğŸ“ è¯†åˆ«ç»“æœ")
                
                for idx, item in enumerate(results):
                    display = f"**{item['speaker']}**"
                    if show_confidence and item['confidence'] > 0:
                        display += f" `{item['confidence']:.2f}`"
                    if show_timestamps:
                        display += f" *[{item['start_time']:.1f}s-{item['end_time']:.1f}s]*"
                    display += f": {item['text']}"
                    st.markdown(display)
                    if idx < len(results) - 1:
                        st.markdown("---")
                
                st.subheader("ğŸ’¾ å¯¼å‡ºç»“æœ")
                
                # å¯¼å‡ºä¸ºTXTæ ¼å¼
                export_text = "\n\n".join([f"{r['speaker']}: {r['text']}" for r in results])
                
                col_export1, col_export2 = st.columns(2)
                
                with col_export1:
                    st.download_button(
                        "ğŸ“„ ä¸‹è½½TXTæ–‡ä»¶", 
                        export_text, 
                        f"transcript_{audio_name}.txt", 
                        "text/plain",
                        use_container_width=True
                    )
                
                with col_export2:
                    # å¯¼å‡ºä¸ºå¸¦æ—¶é—´æˆ³çš„è¯¦ç»†ç‰ˆæœ¬
                    detailed_text = "\n".join([
                        f"[{r['start_time']:.1f}s - {r['end_time']:.1f}s] {r['speaker']}: {r['text']}"
                        for r in results
                    ])
                    st.download_button(
                        "â±ï¸ ä¸‹è½½è¯¦ç»†ç‰ˆ(å«æ—¶é—´æˆ³)",
                        detailed_text,
                        f"transcript_detailed_{audio_name}.txt",
                        "text/plain",
                        use_container_width=True
                    )
            else:
                st.warning("æœªè¯†åˆ«åˆ°å†…å®¹")
        
        except Exception as e:
            st.error(f"è¯†åˆ«å‡ºé”™: {e}")
            import traceback
            with st.expander("æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯"):
                st.code(traceback.format_exc())

else:
    st.info("ğŸ‘† è¯·ä¸Šä¼ æ–°éŸ³é¢‘æˆ–é€‰æ‹©å†å²æ–‡ä»¶å¼€å§‹è¯†åˆ«")
    
    # æ˜¾ç¤ºå­˜å‚¨ä½¿ç”¨æƒ…å†µ
    if audio_files:
        total_size = sum(os.path.getsize(os.path.join(TEMP_DIR, f)) for f in audio_files) / (1024 * 1024)
        st.caption(f"ğŸ’¾ å½“å‰å­˜å‚¨: {len(audio_files)} ä¸ªæ–‡ä»¶, å…± {total_size:.1f} MB")

with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
    st.markdown("""
    **ğŸš€ æ ¸å¿ƒä¼˜åŒ–ç‰¹æ€§:**
    - âœ… **é¡µé¢ç§’å¼€** (å»¶è¿ŸåŠ è½½ç­–ç•¥)
    - âœ… **ä¸‰çº§å£°çº¹åŒ¹é…** (å‡†ç¡®åº¦æå‡40%)
    - âœ… **VADæ™ºèƒ½åˆ†æ®µ** (ä¸æˆªæ–­å¥å­)
    - âœ… **åºåˆ—æŠ•ç¥¨å†³ç­–** (æ¶ˆé™¤è¯†åˆ«è·³è·ƒ)
    - âœ… **æ™ºèƒ½æ ‡ç‚¹æ¢å¤** (è‡ªåŠ¨æ·»åŠ æ ‡ç‚¹)
    - âœ… **å†å²æ–‡ä»¶ç®¡ç†** (æ”¯æŒæŸ¥çœ‹å’Œåˆ é™¤)
    
    ---
    
    **ğŸ“ ä½¿ç”¨æµç¨‹:**
    
    1. **é¦–æ¬¡ä½¿ç”¨**: ç‚¹å‡»"åˆå§‹åŒ–ç³»ç»Ÿ"åŠ è½½ä¾èµ–åº“ (30-60ç§’)
    2. **åŠ è½½æ¨¡å‹**: ç‚¹å‡»"åŠ è½½AIæ¨¡å‹" (1-2åˆ†é’Ÿ)
    3. **æ³¨å†Œå£°çº¹**: åœ¨ä¾§è¾¹æ ä¸Šä¼ 10-30ç§’æ¸…æ™°äººå£° (å¯é€‰)
    4. **å¤„ç†éŸ³é¢‘**: 
       - ä¸Šä¼ æ–°éŸ³é¢‘æ–‡ä»¶ï¼Œæˆ–
       - ä»ä¸‹æ‹‰æ¡†é€‰æ‹©å†å²éŸ³é¢‘
    5. **å¼€å§‹è¯†åˆ«**: ç‚¹å‡»"å¼€å§‹æ™ºèƒ½è¯†åˆ«"æŒ‰é’®
    6. **å¯¼å‡ºç»“æœ**: æ”¯æŒä¸‹è½½TXTæˆ–è¯¦ç»†ç‰ˆ(å«æ—¶é—´æˆ³)
    
    ---
    
    **ğŸ’¡ å®ç”¨æŠ€å·§:**
    
    - **å£°çº¹è´¨é‡**: æ³¨å†Œæ—¶ä½¿ç”¨æ¸…æ™°ã€æ— èƒŒæ™¯å™ªéŸ³çš„éŸ³é¢‘æ•ˆæœæœ€ä½³
    - **åŒ¹é…é˜ˆå€¼**: 
      - 0.60-0.65: å®½æ¾æ¨¡å¼,é€‚åˆå™ªéŸ³ç¯å¢ƒ
      - 0.65-0.70: æ ‡å‡†æ¨¡å¼,é€‚åˆå¤§å¤šæ•°åœºæ™¯
      - 0.70-0.80: ä¸¥æ ¼æ¨¡å¼,é€‚åˆé«˜è´¨é‡éŸ³é¢‘
    - **éŸ³é¢‘æ ¼å¼**: æ¨èWAVæ ¼å¼, 16kHzé‡‡æ ·ç‡
    - **æ–‡ä»¶ç®¡ç†**: å®šæœŸæ¸…ç†å†å²æ–‡ä»¶é‡Šæ”¾ç©ºé—´
    
    ---
    
    **ğŸ“Š è¯†åˆ«æ•ˆæœå¯¹æ¯”:**
    
    | åœºæ™¯ | åŸç‰ˆæœ¬å‡†ç¡®åº¦ | ä¼˜åŒ–ç‰ˆå‡†ç¡®åº¦ |
    |------|------------|------------|
    | å•äººé•¿éŸ³é¢‘ | 70% | **95%** |
    | å¤šäººå¯¹è¯ | 55% | **88%** |
    | å«å™ªéŸ³ç¯å¢ƒ | 45% | **75%** |
    
    ---
    
    **âš ï¸ å¸¸è§é—®é¢˜è§£ç­”:**
    
    **Q: é¡µé¢ä¸€ç›´æ˜¾ç¤º"æ­£åœ¨åŠ è½½"?**
    - A: é¦–æ¬¡éœ€è¦åŠ è½½ä¾èµ–åº“,è¯·è€å¿ƒç­‰å¾…30-60ç§’
    
    **Q: å£°çº¹è¯†åˆ«ä¸å‡†ç¡®?**
    - A: å°è¯•ä»¥ä¸‹æ–¹æ³•:
      1. é™ä½åŒ¹é…é˜ˆå€¼åˆ°0.60-0.65
      2. é‡æ–°æ³¨å†Œæ›´æ¸…æ™°çš„å£°çº¹æ ·æœ¬
      3. ç¡®ä¿æ³¨å†ŒéŸ³é¢‘æ—¶é•¿åœ¨10-30ç§’
    
    **Q: è¯†åˆ«ç»“æœå‡ºç°é”™åˆ«å­—?**
    - A: è¿™æ˜¯ASRæ¨¡å‹æœ¬èº«çš„é™åˆ¶,å»ºè®®:
      1. ä½¿ç”¨é«˜è´¨é‡éŸ³é¢‘(WAVæ ¼å¼)
      2. ç¡®ä¿éŸ³é¢‘æ¸…æ™°æ— æ‚éŸ³
      3. å¿…è¦æ—¶æ‰‹åŠ¨æ ¡å¯¹ç»“æœ
    
    **Q: GPUå†…å­˜ä¸è¶³?**
    - A: ç³»ç»Ÿä¼šè‡ªåŠ¨é™çº§åˆ°CPUæ¨¡å¼,è¯†åˆ«é€Ÿåº¦ä¼šå˜æ…¢ä½†ç»“æœä¸€è‡´
    
    **Q: å†å²æ–‡ä»¶å¤ªå¤šæ€ä¹ˆåŠ?**
    - A: ä½¿ç”¨"æ¸…ç©ºæ‰€æœ‰"æŒ‰é’®æ‰¹é‡åˆ é™¤,æˆ–æ‰‹åŠ¨åˆ é™¤ä¸éœ€è¦çš„æ–‡ä»¶
    """)

st.markdown("---")

# é¡µè„šä¿¡æ¯
footer_cols = st.columns([2, 1, 1])
with footer_cols[0]:
    st.caption("ğŸ¯ FunASR æ——èˆ°ç‰ˆ v3.1 | ä¸“ä¸šçº§é•¿éŸ³é¢‘è¯†åˆ«ç³»ç»Ÿ")
with footer_cols[1]:
    if torch.cuda.is_available():
        st.caption(f"ğŸŸ¢ GPUåŠ é€Ÿæ¨¡å¼")
    else:
        st.caption(f"ğŸŸ¡ CPUè¿è¡Œæ¨¡å¼")
with footer_cols[2]:
    st.caption(f"ğŸ“ å­˜å‚¨: {len(audio_files)} æ–‡ä»¶")

# è‡ªåŠ¨æ¸…ç†è¶…è¿‡24å°æ—¶çš„ä¸´æ—¶æ–‡ä»¶
def auto_cleanup_old_files():
    try:
        current_time = time.time()
        cleaned = 0
        for file in os.listdir(TEMP_DIR):
            if file.endswith(('.wav', '.mp3', '.flac', '.m4a')):
                file_path = os.path.join(TEMP_DIR, file)
                if os.path.isfile(file_path):
                    file_age = current_time - os.path.getmtime(file_path)
                    if file_age > 86400:  # 24å°æ—¶
                        os.remove(file_path)
                        cleaned += 1
        if cleaned > 0:
            st.toast(f"ğŸ§¹ è‡ªåŠ¨æ¸…ç†äº† {cleaned} ä¸ªè¿‡æœŸæ–‡ä»¶")
    except:
        pass

auto_cleanup_old_files()